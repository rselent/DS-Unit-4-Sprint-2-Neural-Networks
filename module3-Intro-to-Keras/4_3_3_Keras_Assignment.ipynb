{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_3_3-Keras-Assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pBQsZEJmubLs"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "\n",
        "# Neural Network Framework (Keras)\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignmnet 3*\n",
        "\n",
        "## Use the Keras Library to build a Multi-Layer Perceptron Model on the Boston Housing dataset\n",
        "\n",
        "- The Boston Housing dataset comes with the Keras library so use Keras to import it into your notebook. \n",
        "- Normalize the data (all features should have roughly the same scale)\n",
        "- Import the type of model and layers that you will need from Keras.\n",
        "- Instantiate a model object and use `model.add()` to add layers to your model\n",
        "- Since this is a regression model you will have a single output node in the final layer.\n",
        "- Use activation functions that are appropriate for this task\n",
        "- Compile your model\n",
        "- Fit your model and report its accuracy in terms of Mean Squared Error\n",
        "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
        "- Run this same data through a linear regression model. Which achieves higher accuracy?\n",
        "- Do a little bit of feature engineering and see how that affects your neural network model. (you will need to change your model to accept more inputs)\n",
        "- After feature engineering, which model sees a greater accuracy boost due to the new features?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8NLTAR87uYJ-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        },
        "outputId": "e27fd352-a52e-4e69-9707-80a7388a11b5"
      },
      "source": [
        "##### Your Code Here #####\n",
        "\n",
        "!pip install tensorflow --upgrade"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 39kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 52.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.27.1)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/23/53ffe290341cd0855d595b0a2e7485932f473798af173bbe3a584b99bb06/tensorboard-2.1.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 51.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow) (45.1.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.7.2)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed tensorboard-2.1.0 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yviyjDrntpeO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import plotly.graph_objects as go\n",
        "from tensorflow.keras.datasets import boston_housing\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Conv2D\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zqlmutEtpPZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "03ac0b4d-16ae-4519-970e-f4052b288f90"
      },
      "source": [
        "(xTrain, yTrain), (xTest, yTest) = boston_housing.load_data()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJoyXS54tpM8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cb4c8e8c-654e-4426-da5a-ca261c8ca77c"
      },
      "source": [
        "xTrain.shape, xTest.shape, yTrain.shape, yTest.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((404, 13), (102, 13), (404,), (102,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at0t8Dsq6ah1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "moodel = Sequential()\n",
        "moodel.add( Dense( 1024, input_shape= (13,)))\n",
        "moodel.add( Activation( \"relu\"))\n",
        "moodel.add( Dropout( .5))\n",
        "moodel.add( Dense( 256))\n",
        "moodel.add( Activation( \"relu\"))\n",
        "moodel.add( Dropout( .5))\n",
        "moodel.add( Dense( 1))\n",
        "moodel.add( Activation( \"linear\"))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nA464WpVJ1w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "ad4b6e6d-b1a4-4823-bb0d-9e52a788c226"
      },
      "source": [
        "help( tf.nn.leaky_relu)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on function leaky_relu in module tensorflow.python.ops.nn_ops:\n",
            "\n",
            "leaky_relu(features, alpha=0.2, name=None)\n",
            "    Compute the Leaky ReLU activation function.\n",
            "    \n",
            "    Source: [Rectifier Nonlinearities Improve Neural Network Acoustic Models.\n",
            "    AL Maas, AY Hannun, AY Ng - Proc. ICML, 2013](https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf).\n",
            "    \n",
            "    Args:\n",
            "      features: A `Tensor` representing preactivation values. Must be one of\n",
            "        the following types: `float16`, `float32`, `float64`, `int32`, `int64`.\n",
            "      alpha: Slope of the activation function at x < 0.\n",
            "      name: A name for the operation (optional).\n",
            "    \n",
            "    Returns:\n",
            "      The activation value.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odPJlaSv-sAe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "4c8dac2f-2625-4ba8-b80a-88d829e64fef"
      },
      "source": [
        "moodel.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 512)               7168      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 129       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 72,961\n",
            "Trainable params: 72,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E3HM2Lb6ae7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "moodel.compile( loss= \"mse\",\n",
        "               optimizer= Adam(),\n",
        "               metrics= [\"mae\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8JODfjR6acO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "848d65e8-aafa-4484-b8a9-66ecd0423d02"
      },
      "source": [
        "mooFit = moodel.fit( xTrain, yTrain,\n",
        "                     batch_size= 512,\n",
        "                     epochs= 1024,\n",
        "                     verbose= 1,\n",
        "                     validation_data= ( xTest, yTest)\n",
        ")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 404 samples, validate on 102 samples\n",
            "Epoch 1/1024\n",
            "404/404 [==============================] - 0s 1ms/sample - loss: 1620.3861 - mae: 32.0601 - val_loss: 7839.0869 - val_mae: 84.5795\n",
            "Epoch 2/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 9019.6084 - mae: 84.1872 - val_loss: 706.4378 - val_mae: 23.4044\n",
            "Epoch 3/1024\n",
            "404/404 [==============================] - 0s 90us/sample - loss: 1640.4323 - mae: 31.7864 - val_loss: 1246.4948 - val_mae: 34.2696\n",
            "Epoch 4/1024\n",
            "404/404 [==============================] - 0s 85us/sample - loss: 2181.3140 - mae: 37.7329 - val_loss: 3115.8281 - val_mae: 54.7772\n",
            "Epoch 5/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 4211.6978 - mae: 56.5299 - val_loss: 2175.7844 - val_mae: 45.7579\n",
            "Epoch 6/1024\n",
            "404/404 [==============================] - 0s 90us/sample - loss: 3010.4470 - mae: 46.9902 - val_loss: 795.3260 - val_mae: 27.0899\n",
            "Epoch 7/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 1446.1012 - mae: 30.8021 - val_loss: 183.1312 - val_mae: 10.7810\n",
            "Epoch 8/1024\n",
            "404/404 [==============================] - 0s 125us/sample - loss: 627.3758 - mae: 19.7280 - val_loss: 167.4042 - val_mae: 10.0810\n",
            "Epoch 9/1024\n",
            "404/404 [==============================] - 0s 111us/sample - loss: 532.5694 - mae: 17.8013 - val_loss: 315.2185 - val_mae: 14.3323\n",
            "Epoch 10/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 886.9534 - mae: 23.3263 - val_loss: 382.1657 - val_mae: 16.0235\n",
            "Epoch 11/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 990.9998 - mae: 25.2480 - val_loss: 338.8252 - val_mae: 14.9632\n",
            "Epoch 12/1024\n",
            "404/404 [==============================] - 0s 108us/sample - loss: 1111.1914 - mae: 26.1757 - val_loss: 231.6735 - val_mae: 12.0100\n",
            "Epoch 13/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 818.2448 - mae: 23.3959 - val_loss: 147.3617 - val_mae: 9.3805\n",
            "Epoch 14/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 543.5454 - mae: 18.3787 - val_loss: 138.0733 - val_mae: 8.9714\n",
            "Epoch 15/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 425.1577 - mae: 16.0933 - val_loss: 195.9430 - val_mae: 11.3485\n",
            "Epoch 16/1024\n",
            "404/404 [==============================] - 0s 105us/sample - loss: 312.7975 - mae: 13.5609 - val_loss: 282.7725 - val_mae: 14.3103\n",
            "Epoch 17/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 278.0151 - mae: 12.8902 - val_loss: 367.3713 - val_mae: 16.9771\n",
            "Epoch 18/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 282.1841 - mae: 13.1878 - val_loss: 439.3306 - val_mae: 19.0583\n",
            "Epoch 19/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 288.4034 - mae: 13.3855 - val_loss: 492.6291 - val_mae: 20.4806\n",
            "Epoch 20/1024\n",
            "404/404 [==============================] - 0s 114us/sample - loss: 326.7229 - mae: 14.4972 - val_loss: 524.4612 - val_mae: 21.2892\n",
            "Epoch 21/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 300.4136 - mae: 13.6608 - val_loss: 536.6572 - val_mae: 21.5991\n",
            "Epoch 22/1024\n",
            "404/404 [==============================] - 0s 111us/sample - loss: 317.8143 - mae: 14.6040 - val_loss: 532.4387 - val_mae: 21.5170\n",
            "Epoch 23/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 346.1587 - mae: 14.9208 - val_loss: 516.7854 - val_mae: 21.1497\n",
            "Epoch 24/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 326.9371 - mae: 14.6712 - val_loss: 491.6346 - val_mae: 20.5377\n",
            "Epoch 25/1024\n",
            "404/404 [==============================] - 0s 105us/sample - loss: 310.7385 - mae: 13.9397 - val_loss: 458.4388 - val_mae: 19.7029\n",
            "Epoch 26/1024\n",
            "404/404 [==============================] - 0s 107us/sample - loss: 302.3000 - mae: 14.2674 - val_loss: 418.7991 - val_mae: 18.6563\n",
            "Epoch 27/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 232.2591 - mae: 11.9693 - val_loss: 377.9434 - val_mae: 17.5192\n",
            "Epoch 28/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 225.4971 - mae: 11.7568 - val_loss: 337.0269 - val_mae: 16.3295\n",
            "Epoch 29/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 217.9580 - mae: 11.6079 - val_loss: 296.3713 - val_mae: 15.0800\n",
            "Epoch 30/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 162.1253 - mae: 9.8568 - val_loss: 258.8836 - val_mae: 13.8803\n",
            "Epoch 31/1024\n",
            "404/404 [==============================] - 0s 120us/sample - loss: 161.3383 - mae: 9.6826 - val_loss: 224.7145 - val_mae: 12.7048\n",
            "Epoch 32/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 155.5315 - mae: 9.5341 - val_loss: 194.5107 - val_mae: 11.5991\n",
            "Epoch 33/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 148.0756 - mae: 9.4923 - val_loss: 170.0545 - val_mae: 10.6167\n",
            "Epoch 34/1024\n",
            "404/404 [==============================] - 0s 126us/sample - loss: 143.4782 - mae: 9.2071 - val_loss: 150.7774 - val_mae: 9.7719\n",
            "Epoch 35/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 144.4056 - mae: 9.3214 - val_loss: 137.0641 - val_mae: 9.1593\n",
            "Epoch 36/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 136.8934 - mae: 9.1955 - val_loss: 128.2133 - val_mae: 8.7611\n",
            "Epoch 37/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 157.2529 - mae: 9.9284 - val_loss: 123.4545 - val_mae: 8.5479\n",
            "Epoch 38/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 162.5164 - mae: 10.0756 - val_loss: 122.4777 - val_mae: 8.5043\n",
            "Epoch 39/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 151.0211 - mae: 9.3696 - val_loss: 125.1953 - val_mae: 8.6252\n",
            "Epoch 40/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 154.6442 - mae: 9.8743 - val_loss: 131.5400 - val_mae: 8.9147\n",
            "Epoch 41/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 153.1024 - mae: 9.7146 - val_loss: 141.1159 - val_mae: 9.3528\n",
            "Epoch 42/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 146.3723 - mae: 9.4000 - val_loss: 152.6116 - val_mae: 9.8772\n",
            "Epoch 43/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 162.3312 - mae: 9.8544 - val_loss: 166.0537 - val_mae: 10.4732\n",
            "Epoch 44/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 127.0024 - mae: 8.6175 - val_loss: 180.8481 - val_mae: 11.0921\n",
            "Epoch 45/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 130.1214 - mae: 8.8941 - val_loss: 197.0358 - val_mae: 11.7292\n",
            "Epoch 46/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 118.6653 - mae: 8.1905 - val_loss: 213.3775 - val_mae: 12.3397\n",
            "Epoch 47/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 125.2634 - mae: 8.6092 - val_loss: 229.7328 - val_mae: 12.9365\n",
            "Epoch 48/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 132.9548 - mae: 8.6839 - val_loss: 244.8809 - val_mae: 13.4668\n",
            "Epoch 49/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 126.8557 - mae: 8.2935 - val_loss: 258.3918 - val_mae: 13.9234\n",
            "Epoch 50/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 121.6249 - mae: 8.1466 - val_loss: 268.6542 - val_mae: 14.2663\n",
            "Epoch 51/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 131.4572 - mae: 8.6562 - val_loss: 275.7597 - val_mae: 14.5069\n",
            "Epoch 52/1024\n",
            "404/404 [==============================] - 0s 108us/sample - loss: 128.3984 - mae: 8.2931 - val_loss: 279.0945 - val_mae: 14.6281\n",
            "Epoch 53/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 106.9127 - mae: 7.8829 - val_loss: 279.3294 - val_mae: 14.6459\n",
            "Epoch 54/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 126.0018 - mae: 8.5468 - val_loss: 277.0532 - val_mae: 14.5818\n",
            "Epoch 55/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 125.0691 - mae: 8.4059 - val_loss: 271.7778 - val_mae: 14.4196\n",
            "Epoch 56/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 114.5868 - mae: 7.9510 - val_loss: 264.5538 - val_mae: 14.1904\n",
            "Epoch 57/1024\n",
            "404/404 [==============================] - 0s 107us/sample - loss: 121.2611 - mae: 8.2695 - val_loss: 256.5215 - val_mae: 13.9272\n",
            "Epoch 58/1024\n",
            "404/404 [==============================] - 0s 116us/sample - loss: 127.3809 - mae: 8.4635 - val_loss: 247.3546 - val_mae: 13.6185\n",
            "Epoch 59/1024\n",
            "404/404 [==============================] - 0s 109us/sample - loss: 108.7159 - mae: 7.7149 - val_loss: 237.5661 - val_mae: 13.2822\n",
            "Epoch 60/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 122.7282 - mae: 8.6425 - val_loss: 228.0269 - val_mae: 12.9488\n",
            "Epoch 61/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 111.4074 - mae: 7.9965 - val_loss: 219.9641 - val_mae: 12.6598\n",
            "Epoch 62/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 109.9633 - mae: 8.0222 - val_loss: 213.4589 - val_mae: 12.4201\n",
            "Epoch 63/1024\n",
            "404/404 [==============================] - 0s 111us/sample - loss: 105.9411 - mae: 7.9318 - val_loss: 207.3898 - val_mae: 12.1918\n",
            "Epoch 64/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 113.2117 - mae: 7.9850 - val_loss: 203.1568 - val_mae: 12.0290\n",
            "Epoch 65/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 105.9996 - mae: 7.7115 - val_loss: 199.7679 - val_mae: 11.8963\n",
            "Epoch 66/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 120.1369 - mae: 8.2470 - val_loss: 198.2732 - val_mae: 11.8364\n",
            "Epoch 67/1024\n",
            "404/404 [==============================] - 0s 108us/sample - loss: 119.7785 - mae: 8.4762 - val_loss: 199.4781 - val_mae: 11.8819\n",
            "Epoch 68/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 114.7503 - mae: 7.9711 - val_loss: 202.3302 - val_mae: 11.9915\n",
            "Epoch 69/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 108.1994 - mae: 7.6619 - val_loss: 206.0762 - val_mae: 12.1337\n",
            "Epoch 70/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 114.4173 - mae: 7.8000 - val_loss: 209.7353 - val_mae: 12.2707\n",
            "Epoch 71/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 114.2792 - mae: 8.2055 - val_loss: 212.9463 - val_mae: 12.3885\n",
            "Epoch 72/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 99.1687 - mae: 7.3272 - val_loss: 215.8058 - val_mae: 12.4919\n",
            "Epoch 73/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 115.3113 - mae: 7.7859 - val_loss: 218.2905 - val_mae: 12.5820\n",
            "Epoch 74/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 109.2211 - mae: 7.8088 - val_loss: 219.8118 - val_mae: 12.6365\n",
            "Epoch 75/1024\n",
            "404/404 [==============================] - 0s 106us/sample - loss: 108.9413 - mae: 7.6656 - val_loss: 220.5798 - val_mae: 12.6633\n",
            "Epoch 76/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 103.0179 - mae: 7.6048 - val_loss: 220.9478 - val_mae: 12.6757\n",
            "Epoch 77/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 113.5980 - mae: 7.8962 - val_loss: 220.4072 - val_mae: 12.6538\n",
            "Epoch 78/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 96.4751 - mae: 7.3762 - val_loss: 220.4119 - val_mae: 12.6524\n",
            "Epoch 79/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 103.3757 - mae: 7.6224 - val_loss: 219.6107 - val_mae: 12.6225\n",
            "Epoch 80/1024\n",
            "404/404 [==============================] - 0s 90us/sample - loss: 104.5076 - mae: 7.3169 - val_loss: 219.2329 - val_mae: 12.6090\n",
            "Epoch 81/1024\n",
            "404/404 [==============================] - 0s 111us/sample - loss: 102.0321 - mae: 7.5427 - val_loss: 219.2554 - val_mae: 12.6107\n",
            "Epoch 82/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 95.8917 - mae: 7.3077 - val_loss: 218.5350 - val_mae: 12.5852\n",
            "Epoch 83/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 101.7861 - mae: 7.5615 - val_loss: 217.2681 - val_mae: 12.5397\n",
            "Epoch 84/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 102.1574 - mae: 7.6254 - val_loss: 215.4397 - val_mae: 12.4730\n",
            "Epoch 85/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 117.0646 - mae: 7.9380 - val_loss: 213.8727 - val_mae: 12.4158\n",
            "Epoch 86/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 105.5962 - mae: 7.7158 - val_loss: 211.5619 - val_mae: 12.3297\n",
            "Epoch 87/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 103.6762 - mae: 7.5282 - val_loss: 209.5983 - val_mae: 12.2558\n",
            "Epoch 88/1024\n",
            "404/404 [==============================] - 0s 112us/sample - loss: 106.2669 - mae: 7.6616 - val_loss: 207.7803 - val_mae: 12.1867\n",
            "Epoch 89/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 102.2845 - mae: 7.4236 - val_loss: 204.7271 - val_mae: 12.0702\n",
            "Epoch 90/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 95.6105 - mae: 7.4129 - val_loss: 202.2027 - val_mae: 11.9733\n",
            "Epoch 91/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 93.8831 - mae: 7.3393 - val_loss: 199.3636 - val_mae: 11.8642\n",
            "Epoch 92/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 95.8706 - mae: 7.4043 - val_loss: 196.8390 - val_mae: 11.7667\n",
            "Epoch 93/1024\n",
            "404/404 [==============================] - 0s 108us/sample - loss: 98.8652 - mae: 7.2716 - val_loss: 194.1985 - val_mae: 11.6631\n",
            "Epoch 94/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 93.7859 - mae: 7.3750 - val_loss: 191.8145 - val_mae: 11.5677\n",
            "Epoch 95/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 87.5835 - mae: 6.9463 - val_loss: 190.1835 - val_mae: 11.5005\n",
            "Epoch 96/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 86.0925 - mae: 6.8280 - val_loss: 187.6493 - val_mae: 11.3973\n",
            "Epoch 97/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 93.8588 - mae: 7.3085 - val_loss: 186.3237 - val_mae: 11.3428\n",
            "Epoch 98/1024\n",
            "404/404 [==============================] - 0s 106us/sample - loss: 100.5874 - mae: 7.6066 - val_loss: 185.1662 - val_mae: 11.2945\n",
            "Epoch 99/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 102.5847 - mae: 7.5475 - val_loss: 184.5437 - val_mae: 11.2678\n",
            "Epoch 100/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 103.3218 - mae: 7.3747 - val_loss: 182.6418 - val_mae: 11.1887\n",
            "Epoch 101/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 105.7401 - mae: 7.3843 - val_loss: 181.9685 - val_mae: 11.1597\n",
            "Epoch 102/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 98.6016 - mae: 7.1649 - val_loss: 181.6639 - val_mae: 11.1456\n",
            "Epoch 103/1024\n",
            "404/404 [==============================] - 0s 112us/sample - loss: 117.0886 - mae: 8.0010 - val_loss: 183.0209 - val_mae: 11.1991\n",
            "Epoch 104/1024\n",
            "404/404 [==============================] - 0s 107us/sample - loss: 95.3998 - mae: 7.1275 - val_loss: 184.9462 - val_mae: 11.2766\n",
            "Epoch 105/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 102.7444 - mae: 7.4383 - val_loss: 186.2287 - val_mae: 11.3287\n",
            "Epoch 106/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 107.7888 - mae: 7.6563 - val_loss: 187.3378 - val_mae: 11.3753\n",
            "Epoch 107/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 97.1871 - mae: 7.2415 - val_loss: 186.8938 - val_mae: 11.3591\n",
            "Epoch 108/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 93.4073 - mae: 7.2614 - val_loss: 186.4486 - val_mae: 11.3422\n",
            "Epoch 109/1024\n",
            "404/404 [==============================] - 0s 105us/sample - loss: 97.6706 - mae: 7.0607 - val_loss: 184.0702 - val_mae: 11.2462\n",
            "Epoch 110/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 97.8782 - mae: 7.1864 - val_loss: 180.4401 - val_mae: 11.0979\n",
            "Epoch 111/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 98.5021 - mae: 7.3247 - val_loss: 176.3417 - val_mae: 10.9277\n",
            "Epoch 112/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 99.5878 - mae: 7.4315 - val_loss: 172.5466 - val_mae: 10.7685\n",
            "Epoch 113/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 89.9143 - mae: 7.1263 - val_loss: 168.0421 - val_mae: 10.5778\n",
            "Epoch 114/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 89.0266 - mae: 7.1144 - val_loss: 163.5600 - val_mae: 10.3844\n",
            "Epoch 115/1024\n",
            "404/404 [==============================] - 0s 105us/sample - loss: 90.1605 - mae: 6.9366 - val_loss: 160.1037 - val_mae: 10.2325\n",
            "Epoch 116/1024\n",
            "404/404 [==============================] - 0s 108us/sample - loss: 89.8168 - mae: 7.0746 - val_loss: 158.1054 - val_mae: 10.1441\n",
            "Epoch 117/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 96.3379 - mae: 7.1586 - val_loss: 156.6062 - val_mae: 10.0778\n",
            "Epoch 118/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 90.2398 - mae: 7.0678 - val_loss: 156.5600 - val_mae: 10.0774\n",
            "Epoch 119/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 80.9975 - mae: 6.6352 - val_loss: 157.0914 - val_mae: 10.1034\n",
            "Epoch 120/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 87.8427 - mae: 7.0669 - val_loss: 159.3903 - val_mae: 10.2076\n",
            "Epoch 121/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 89.3461 - mae: 6.9958 - val_loss: 163.3701 - val_mae: 10.3837\n",
            "Epoch 122/1024\n",
            "404/404 [==============================] - 0s 116us/sample - loss: 95.3907 - mae: 7.1818 - val_loss: 167.0208 - val_mae: 10.5425\n",
            "Epoch 123/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 81.5213 - mae: 6.6843 - val_loss: 168.9092 - val_mae: 10.6238\n",
            "Epoch 124/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 98.2339 - mae: 7.2114 - val_loss: 169.0837 - val_mae: 10.6317\n",
            "Epoch 125/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 92.4699 - mae: 6.9857 - val_loss: 166.0972 - val_mae: 10.5030\n",
            "Epoch 126/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 94.1810 - mae: 7.2295 - val_loss: 163.0523 - val_mae: 10.3710\n",
            "Epoch 127/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 93.7246 - mae: 7.0863 - val_loss: 160.4244 - val_mae: 10.2562\n",
            "Epoch 128/1024\n",
            "404/404 [==============================] - 0s 110us/sample - loss: 87.5733 - mae: 6.6940 - val_loss: 158.2673 - val_mae: 10.1613\n",
            "Epoch 129/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 95.6908 - mae: 7.0232 - val_loss: 155.4734 - val_mae: 10.0359\n",
            "Epoch 130/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 97.8164 - mae: 7.3992 - val_loss: 152.7210 - val_mae: 9.9115\n",
            "Epoch 131/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 93.4526 - mae: 7.1696 - val_loss: 151.1708 - val_mae: 9.8414\n",
            "Epoch 132/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 88.9091 - mae: 6.9126 - val_loss: 150.0183 - val_mae: 9.7889\n",
            "Epoch 133/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 98.6332 - mae: 7.5174 - val_loss: 149.6034 - val_mae: 9.7713\n",
            "Epoch 134/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 93.5574 - mae: 7.0298 - val_loss: 150.0449 - val_mae: 9.7935\n",
            "Epoch 135/1024\n",
            "404/404 [==============================] - 0s 121us/sample - loss: 84.4432 - mae: 6.7668 - val_loss: 149.9884 - val_mae: 9.7923\n",
            "Epoch 136/1024\n",
            "404/404 [==============================] - 0s 114us/sample - loss: 87.0402 - mae: 6.7797 - val_loss: 150.4165 - val_mae: 9.8145\n",
            "Epoch 137/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 93.6882 - mae: 7.0759 - val_loss: 151.5638 - val_mae: 9.8712\n",
            "Epoch 138/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 96.9980 - mae: 7.2211 - val_loss: 151.5824 - val_mae: 9.8750\n",
            "Epoch 139/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 85.4645 - mae: 6.7027 - val_loss: 151.3486 - val_mae: 9.8671\n",
            "Epoch 140/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 87.5586 - mae: 6.9670 - val_loss: 150.6057 - val_mae: 9.8347\n",
            "Epoch 141/1024\n",
            "404/404 [==============================] - 0s 109us/sample - loss: 87.1944 - mae: 6.8290 - val_loss: 148.6773 - val_mae: 9.7459\n",
            "Epoch 142/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 84.8005 - mae: 6.8054 - val_loss: 145.8765 - val_mae: 9.6146\n",
            "Epoch 143/1024\n",
            "404/404 [==============================] - 0s 90us/sample - loss: 86.6237 - mae: 6.8371 - val_loss: 142.8316 - val_mae: 9.4727\n",
            "Epoch 144/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 88.2765 - mae: 6.7691 - val_loss: 139.3059 - val_mae: 9.3085\n",
            "Epoch 145/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 95.7949 - mae: 6.9430 - val_loss: 137.0273 - val_mae: 9.2036\n",
            "Epoch 146/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 83.8713 - mae: 6.6301 - val_loss: 135.6298 - val_mae: 9.1395\n",
            "Epoch 147/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 89.0323 - mae: 7.2065 - val_loss: 135.8025 - val_mae: 9.1488\n",
            "Epoch 148/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 87.5767 - mae: 7.0618 - val_loss: 137.2828 - val_mae: 9.2206\n",
            "Epoch 149/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 86.8757 - mae: 6.8193 - val_loss: 138.9615 - val_mae: 9.3006\n",
            "Epoch 150/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 79.4886 - mae: 6.5892 - val_loss: 140.0624 - val_mae: 9.3538\n",
            "Epoch 151/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 83.6573 - mae: 6.6867 - val_loss: 140.1170 - val_mae: 9.3573\n",
            "Epoch 152/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 82.6678 - mae: 6.7457 - val_loss: 139.0952 - val_mae: 9.3097\n",
            "Epoch 153/1024\n",
            "404/404 [==============================] - 0s 90us/sample - loss: 94.0862 - mae: 7.0728 - val_loss: 137.7093 - val_mae: 9.2439\n",
            "Epoch 154/1024\n",
            "404/404 [==============================] - 0s 90us/sample - loss: 89.5588 - mae: 7.0589 - val_loss: 136.0104 - val_mae: 9.1628\n",
            "Epoch 155/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 86.1488 - mae: 6.7514 - val_loss: 132.6189 - val_mae: 9.0021\n",
            "Epoch 156/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 86.6392 - mae: 6.9668 - val_loss: 130.6242 - val_mae: 8.9075\n",
            "Epoch 157/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 85.0332 - mae: 6.6558 - val_loss: 130.0782 - val_mae: 8.8837\n",
            "Epoch 158/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 80.1594 - mae: 6.4186 - val_loss: 129.6451 - val_mae: 8.8653\n",
            "Epoch 159/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 84.4261 - mae: 6.8074 - val_loss: 130.7292 - val_mae: 8.9173\n",
            "Epoch 160/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 84.1405 - mae: 6.9017 - val_loss: 130.8120 - val_mae: 8.9225\n",
            "Epoch 161/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 89.4542 - mae: 7.0166 - val_loss: 130.5747 - val_mae: 8.9128\n",
            "Epoch 162/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 84.7533 - mae: 6.6304 - val_loss: 130.6052 - val_mae: 8.9149\n",
            "Epoch 163/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 86.5588 - mae: 6.5639 - val_loss: 131.4504 - val_mae: 8.9538\n",
            "Epoch 164/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 89.6563 - mae: 7.1519 - val_loss: 133.2427 - val_mae: 9.0377\n",
            "Epoch 165/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 77.2816 - mae: 6.3777 - val_loss: 133.5384 - val_mae: 9.0525\n",
            "Epoch 166/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 85.8683 - mae: 6.8027 - val_loss: 131.6195 - val_mae: 8.9645\n",
            "Epoch 167/1024\n",
            "404/404 [==============================] - 0s 116us/sample - loss: 82.4494 - mae: 6.6468 - val_loss: 128.9217 - val_mae: 8.8399\n",
            "Epoch 168/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 83.0107 - mae: 6.5559 - val_loss: 125.8069 - val_mae: 8.6932\n",
            "Epoch 169/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 89.3214 - mae: 6.9776 - val_loss: 123.9615 - val_mae: 8.6042\n",
            "Epoch 170/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 84.5435 - mae: 6.6961 - val_loss: 122.6441 - val_mae: 8.5412\n",
            "Epoch 171/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 85.3608 - mae: 6.8126 - val_loss: 122.1264 - val_mae: 8.5159\n",
            "Epoch 172/1024\n",
            "404/404 [==============================] - 0s 90us/sample - loss: 81.3735 - mae: 6.7723 - val_loss: 123.5774 - val_mae: 8.5848\n",
            "Epoch 173/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 84.0085 - mae: 6.6782 - val_loss: 125.9520 - val_mae: 8.6996\n",
            "Epoch 174/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 81.5452 - mae: 6.5596 - val_loss: 127.7187 - val_mae: 8.7841\n",
            "Epoch 175/1024\n",
            "404/404 [==============================] - 0s 112us/sample - loss: 78.2966 - mae: 6.5975 - val_loss: 129.3492 - val_mae: 8.8615\n",
            "Epoch 176/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 77.4070 - mae: 6.4009 - val_loss: 130.8145 - val_mae: 8.9313\n",
            "Epoch 177/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 91.7685 - mae: 6.7313 - val_loss: 130.9614 - val_mae: 8.9380\n",
            "Epoch 178/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 77.5120 - mae: 6.5035 - val_loss: 130.0024 - val_mae: 8.8929\n",
            "Epoch 179/1024\n",
            "404/404 [==============================] - 0s 108us/sample - loss: 89.1998 - mae: 6.8623 - val_loss: 127.8244 - val_mae: 8.7929\n",
            "Epoch 180/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 88.5666 - mae: 6.8406 - val_loss: 124.7503 - val_mae: 8.6499\n",
            "Epoch 181/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 77.3334 - mae: 6.3474 - val_loss: 121.0083 - val_mae: 8.4715\n",
            "Epoch 182/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 72.9465 - mae: 6.3622 - val_loss: 116.9802 - val_mae: 8.2786\n",
            "Epoch 183/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 86.2437 - mae: 6.8548 - val_loss: 114.1444 - val_mae: 8.1447\n",
            "Epoch 184/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 83.5139 - mae: 6.6980 - val_loss: 112.4214 - val_mae: 8.0652\n",
            "Epoch 185/1024\n",
            "404/404 [==============================] - 0s 117us/sample - loss: 82.3818 - mae: 6.6534 - val_loss: 112.3045 - val_mae: 8.0642\n",
            "Epoch 186/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 74.3758 - mae: 6.3469 - val_loss: 113.3738 - val_mae: 8.1170\n",
            "Epoch 187/1024\n",
            "404/404 [==============================] - 0s 106us/sample - loss: 79.9919 - mae: 6.6377 - val_loss: 116.6336 - val_mae: 8.2754\n",
            "Epoch 188/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 78.7888 - mae: 6.4755 - val_loss: 120.0014 - val_mae: 8.4384\n",
            "Epoch 189/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 90.1402 - mae: 7.0135 - val_loss: 123.6789 - val_mae: 8.6182\n",
            "Epoch 190/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 82.1603 - mae: 6.7432 - val_loss: 126.8044 - val_mae: 8.7682\n",
            "Epoch 191/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 87.2085 - mae: 6.8452 - val_loss: 127.8834 - val_mae: 8.8195\n",
            "Epoch 192/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 81.4319 - mae: 6.4935 - val_loss: 127.6497 - val_mae: 8.8091\n",
            "Epoch 193/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 88.5592 - mae: 6.9797 - val_loss: 125.0462 - val_mae: 8.6856\n",
            "Epoch 194/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 80.6382 - mae: 6.3439 - val_loss: 120.2403 - val_mae: 8.4539\n",
            "Epoch 195/1024\n",
            "404/404 [==============================] - 0s 88us/sample - loss: 81.2639 - mae: 6.6390 - val_loss: 116.1218 - val_mae: 8.2575\n",
            "Epoch 196/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 80.6135 - mae: 6.5649 - val_loss: 113.1614 - val_mae: 8.1223\n",
            "Epoch 197/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 76.1206 - mae: 6.4895 - val_loss: 110.6255 - val_mae: 8.0048\n",
            "Epoch 198/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 78.4057 - mae: 6.5017 - val_loss: 109.0826 - val_mae: 7.9341\n",
            "Epoch 199/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 80.6009 - mae: 6.5978 - val_loss: 109.0029 - val_mae: 7.9296\n",
            "Epoch 200/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 79.3142 - mae: 6.5664 - val_loss: 113.1355 - val_mae: 8.1194\n",
            "Epoch 201/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 76.8191 - mae: 6.5308 - val_loss: 117.8978 - val_mae: 8.3468\n",
            "Epoch 202/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 84.2798 - mae: 6.8608 - val_loss: 121.2701 - val_mae: 8.5155\n",
            "Epoch 203/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 73.3481 - mae: 6.1474 - val_loss: 124.1157 - val_mae: 8.6566\n",
            "Epoch 204/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 77.7677 - mae: 6.5554 - val_loss: 125.6012 - val_mae: 8.7279\n",
            "Epoch 205/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 86.5990 - mae: 6.9129 - val_loss: 125.2669 - val_mae: 8.7129\n",
            "Epoch 206/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 77.4137 - mae: 6.4248 - val_loss: 122.2477 - val_mae: 8.5693\n",
            "Epoch 207/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 78.1201 - mae: 6.6559 - val_loss: 116.3984 - val_mae: 8.2820\n",
            "Epoch 208/1024\n",
            "404/404 [==============================] - 0s 89us/sample - loss: 80.4977 - mae: 6.4999 - val_loss: 109.4568 - val_mae: 7.9383\n",
            "Epoch 209/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 71.1920 - mae: 6.1637 - val_loss: 103.3882 - val_mae: 7.6500\n",
            "Epoch 210/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 86.4532 - mae: 6.5930 - val_loss: 99.5403 - val_mae: 7.4695\n",
            "Epoch 211/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 80.5603 - mae: 6.7151 - val_loss: 101.0133 - val_mae: 7.5420\n",
            "Epoch 212/1024\n",
            "404/404 [==============================] - 0s 112us/sample - loss: 77.7653 - mae: 6.5059 - val_loss: 104.8598 - val_mae: 7.7242\n",
            "Epoch 213/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 78.6627 - mae: 6.4725 - val_loss: 110.8946 - val_mae: 8.0228\n",
            "Epoch 214/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 74.4512 - mae: 6.1452 - val_loss: 115.0941 - val_mae: 8.2391\n",
            "Epoch 215/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 71.1387 - mae: 6.0792 - val_loss: 117.3298 - val_mae: 8.3538\n",
            "Epoch 216/1024\n",
            "404/404 [==============================] - 0s 89us/sample - loss: 82.5103 - mae: 6.7111 - val_loss: 118.2205 - val_mae: 8.3996\n",
            "Epoch 217/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 86.9730 - mae: 6.7658 - val_loss: 116.1658 - val_mae: 8.2975\n",
            "Epoch 218/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 80.4889 - mae: 6.6777 - val_loss: 112.3420 - val_mae: 8.1083\n",
            "Epoch 219/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 84.4829 - mae: 6.6501 - val_loss: 107.7140 - val_mae: 7.8976\n",
            "Epoch 220/1024\n",
            "404/404 [==============================] - 0s 153us/sample - loss: 70.5644 - mae: 6.1655 - val_loss: 102.5689 - val_mae: 7.6625\n",
            "Epoch 221/1024\n",
            "404/404 [==============================] - 0s 109us/sample - loss: 75.4685 - mae: 6.3147 - val_loss: 98.0788 - val_mae: 7.4451\n",
            "Epoch 222/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 78.9500 - mae: 6.5185 - val_loss: 98.6826 - val_mae: 7.4785\n",
            "Epoch 223/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 77.2522 - mae: 6.4453 - val_loss: 101.0201 - val_mae: 7.5969\n",
            "Epoch 224/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 71.9406 - mae: 6.1907 - val_loss: 104.0590 - val_mae: 7.7453\n",
            "Epoch 225/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 77.9833 - mae: 6.4038 - val_loss: 106.9763 - val_mae: 7.8832\n",
            "Epoch 226/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 78.0946 - mae: 6.3924 - val_loss: 109.1830 - val_mae: 7.9853\n",
            "Epoch 227/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 78.1799 - mae: 6.4893 - val_loss: 111.9083 - val_mae: 8.1062\n",
            "Epoch 228/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 81.6151 - mae: 6.6296 - val_loss: 112.2774 - val_mae: 8.1257\n",
            "Epoch 229/1024\n",
            "404/404 [==============================] - 0s 112us/sample - loss: 70.2487 - mae: 6.0167 - val_loss: 111.8056 - val_mae: 8.1065\n",
            "Epoch 230/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 78.1938 - mae: 6.4868 - val_loss: 108.7544 - val_mae: 7.9611\n",
            "Epoch 231/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 80.8922 - mae: 6.3591 - val_loss: 104.9056 - val_mae: 7.7814\n",
            "Epoch 232/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 70.2902 - mae: 6.1938 - val_loss: 99.5860 - val_mae: 7.5237\n",
            "Epoch 233/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 69.6822 - mae: 6.2245 - val_loss: 94.0509 - val_mae: 7.2480\n",
            "Epoch 234/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 67.7310 - mae: 5.9809 - val_loss: 89.8156 - val_mae: 7.0343\n",
            "Epoch 235/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 77.5632 - mae: 6.4927 - val_loss: 88.1760 - val_mae: 6.9507\n",
            "Epoch 236/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 67.8138 - mae: 5.9623 - val_loss: 87.9756 - val_mae: 6.9420\n",
            "Epoch 237/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 78.1364 - mae: 6.6183 - val_loss: 91.4367 - val_mae: 7.1215\n",
            "Epoch 238/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 76.7797 - mae: 6.2969 - val_loss: 95.3904 - val_mae: 7.3202\n",
            "Epoch 239/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 73.6876 - mae: 6.1887 - val_loss: 98.4891 - val_mae: 7.4724\n",
            "Epoch 240/1024\n",
            "404/404 [==============================] - 0s 109us/sample - loss: 78.5238 - mae: 6.2691 - val_loss: 100.1917 - val_mae: 7.5554\n",
            "Epoch 241/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 78.3606 - mae: 6.1904 - val_loss: 99.7076 - val_mae: 7.5353\n",
            "Epoch 242/1024\n",
            "404/404 [==============================] - 0s 89us/sample - loss: 74.4358 - mae: 6.2019 - val_loss: 98.2531 - val_mae: 7.4680\n",
            "Epoch 243/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 71.5227 - mae: 6.0198 - val_loss: 96.5001 - val_mae: 7.3852\n",
            "Epoch 244/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 74.1521 - mae: 6.3148 - val_loss: 95.1673 - val_mae: 7.3226\n",
            "Epoch 245/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 71.6763 - mae: 6.0214 - val_loss: 93.9503 - val_mae: 7.2641\n",
            "Epoch 246/1024\n",
            "404/404 [==============================] - 0s 108us/sample - loss: 78.1476 - mae: 6.3919 - val_loss: 93.2476 - val_mae: 7.2301\n",
            "Epoch 247/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 79.5791 - mae: 6.6050 - val_loss: 93.0648 - val_mae: 7.2216\n",
            "Epoch 248/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 78.7827 - mae: 6.5270 - val_loss: 94.6345 - val_mae: 7.3010\n",
            "Epoch 249/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 70.6409 - mae: 5.8839 - val_loss: 95.6564 - val_mae: 7.3555\n",
            "Epoch 250/1024\n",
            "404/404 [==============================] - 0s 108us/sample - loss: 70.3315 - mae: 6.0323 - val_loss: 96.1300 - val_mae: 7.3834\n",
            "Epoch 251/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 71.1150 - mae: 6.1039 - val_loss: 95.0609 - val_mae: 7.3323\n",
            "Epoch 252/1024\n",
            "404/404 [==============================] - 0s 105us/sample - loss: 78.5854 - mae: 6.2641 - val_loss: 93.0500 - val_mae: 7.2332\n",
            "Epoch 253/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 79.6614 - mae: 6.6347 - val_loss: 91.6730 - val_mae: 7.1647\n",
            "Epoch 254/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 67.5659 - mae: 6.0644 - val_loss: 91.0616 - val_mae: 7.1350\n",
            "Epoch 255/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 75.5025 - mae: 6.2122 - val_loss: 90.8546 - val_mae: 7.1254\n",
            "Epoch 256/1024\n",
            "404/404 [==============================] - 0s 121us/sample - loss: 75.9557 - mae: 6.4732 - val_loss: 91.0532 - val_mae: 7.1358\n",
            "Epoch 257/1024\n",
            "404/404 [==============================] - 0s 106us/sample - loss: 75.8857 - mae: 6.3543 - val_loss: 89.7228 - val_mae: 7.0694\n",
            "Epoch 258/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 64.5382 - mae: 5.8596 - val_loss: 88.6149 - val_mae: 7.0161\n",
            "Epoch 259/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 79.7956 - mae: 6.5700 - val_loss: 89.2207 - val_mae: 7.0504\n",
            "Epoch 260/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 81.0181 - mae: 6.7145 - val_loss: 89.7511 - val_mae: 7.0806\n",
            "Epoch 261/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 71.7974 - mae: 6.2460 - val_loss: 89.4371 - val_mae: 7.0677\n",
            "Epoch 262/1024\n",
            "404/404 [==============================] - 0s 107us/sample - loss: 73.4115 - mae: 6.0780 - val_loss: 89.3772 - val_mae: 7.0678\n",
            "Epoch 263/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 72.9133 - mae: 6.2462 - val_loss: 91.6678 - val_mae: 7.1868\n",
            "Epoch 264/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 72.1981 - mae: 6.0267 - val_loss: 94.2144 - val_mae: 7.3177\n",
            "Epoch 265/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 74.7829 - mae: 6.4040 - val_loss: 94.2531 - val_mae: 7.3231\n",
            "Epoch 266/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 72.8569 - mae: 6.0974 - val_loss: 92.5076 - val_mae: 7.2407\n",
            "Epoch 267/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 67.6973 - mae: 6.1448 - val_loss: 88.7603 - val_mae: 7.0525\n",
            "Epoch 268/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 67.7067 - mae: 5.8922 - val_loss: 85.4065 - val_mae: 6.8782\n",
            "Epoch 269/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 74.5091 - mae: 6.2319 - val_loss: 83.1670 - val_mae: 6.7625\n",
            "Epoch 270/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 71.0958 - mae: 6.4038 - val_loss: 82.9706 - val_mae: 6.7539\n",
            "Epoch 271/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 74.0165 - mae: 6.1703 - val_loss: 84.1086 - val_mae: 6.8120\n",
            "Epoch 272/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 71.4726 - mae: 6.3020 - val_loss: 87.1657 - val_mae: 6.9746\n",
            "Epoch 273/1024\n",
            "404/404 [==============================] - 0s 105us/sample - loss: 67.9885 - mae: 6.0824 - val_loss: 90.8405 - val_mae: 7.1660\n",
            "Epoch 274/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 68.2385 - mae: 6.0145 - val_loss: 92.5568 - val_mae: 7.2503\n",
            "Epoch 275/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 67.1059 - mae: 5.9375 - val_loss: 91.3664 - val_mae: 7.1901\n",
            "Epoch 276/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 67.9937 - mae: 5.8187 - val_loss: 87.5573 - val_mae: 6.9897\n",
            "Epoch 277/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 68.2821 - mae: 5.8579 - val_loss: 81.4749 - val_mae: 6.6730\n",
            "Epoch 278/1024\n",
            "404/404 [==============================] - 0s 90us/sample - loss: 66.7539 - mae: 6.0140 - val_loss: 77.2579 - val_mae: 6.4604\n",
            "Epoch 279/1024\n",
            "404/404 [==============================] - 0s 108us/sample - loss: 75.9224 - mae: 6.3106 - val_loss: 76.3387 - val_mae: 6.4119\n",
            "Epoch 280/1024\n",
            "404/404 [==============================] - 0s 111us/sample - loss: 76.8996 - mae: 6.2545 - val_loss: 78.2325 - val_mae: 6.5097\n",
            "Epoch 281/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 72.0446 - mae: 6.3154 - val_loss: 83.4961 - val_mae: 6.7762\n",
            "Epoch 282/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 68.5458 - mae: 6.0210 - val_loss: 88.4714 - val_mae: 7.0367\n",
            "Epoch 283/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 66.8191 - mae: 5.9836 - val_loss: 89.6736 - val_mae: 7.1024\n",
            "Epoch 284/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 75.6498 - mae: 6.1903 - val_loss: 89.4079 - val_mae: 7.0926\n",
            "Epoch 285/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 76.7014 - mae: 6.3167 - val_loss: 85.3075 - val_mae: 6.8863\n",
            "Epoch 286/1024\n",
            "404/404 [==============================] - 0s 108us/sample - loss: 64.1538 - mae: 5.7271 - val_loss: 82.2437 - val_mae: 6.7248\n",
            "Epoch 287/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 63.2669 - mae: 5.8307 - val_loss: 80.7834 - val_mae: 6.6473\n",
            "Epoch 288/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 59.8531 - mae: 5.6584 - val_loss: 80.1937 - val_mae: 6.6164\n",
            "Epoch 289/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 66.6410 - mae: 5.7503 - val_loss: 80.1681 - val_mae: 6.6152\n",
            "Epoch 290/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 68.4405 - mae: 5.9696 - val_loss: 81.6595 - val_mae: 6.6958\n",
            "Epoch 291/1024\n",
            "404/404 [==============================] - 0s 88us/sample - loss: 70.7885 - mae: 6.0763 - val_loss: 82.5270 - val_mae: 6.7401\n",
            "Epoch 292/1024\n",
            "404/404 [==============================] - 0s 106us/sample - loss: 68.2133 - mae: 6.0254 - val_loss: 82.0744 - val_mae: 6.7147\n",
            "Epoch 293/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 63.3349 - mae: 5.7671 - val_loss: 80.6843 - val_mae: 6.6398\n",
            "Epoch 294/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 60.3141 - mae: 5.7129 - val_loss: 80.9923 - val_mae: 6.6583\n",
            "Epoch 295/1024\n",
            "404/404 [==============================] - 0s 107us/sample - loss: 69.2776 - mae: 6.2121 - val_loss: 83.9975 - val_mae: 6.8224\n",
            "Epoch 296/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 67.9573 - mae: 5.8226 - val_loss: 86.3905 - val_mae: 6.9531\n",
            "Epoch 297/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 66.0990 - mae: 5.7715 - val_loss: 83.5137 - val_mae: 6.8038\n",
            "Epoch 298/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 65.8962 - mae: 5.7912 - val_loss: 79.3011 - val_mae: 6.5803\n",
            "Epoch 299/1024\n",
            "404/404 [==============================] - 0s 121us/sample - loss: 64.8234 - mae: 5.9625 - val_loss: 77.7787 - val_mae: 6.5037\n",
            "Epoch 300/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 67.5865 - mae: 6.1822 - val_loss: 77.6880 - val_mae: 6.5061\n",
            "Epoch 301/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 64.6828 - mae: 6.0884 - val_loss: 79.4178 - val_mae: 6.6037\n",
            "Epoch 302/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 63.2339 - mae: 5.6655 - val_loss: 78.7889 - val_mae: 6.5759\n",
            "Epoch 303/1024\n",
            "404/404 [==============================] - 0s 106us/sample - loss: 69.8501 - mae: 5.9972 - val_loss: 78.3566 - val_mae: 6.5576\n",
            "Epoch 304/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 57.1402 - mae: 5.4903 - val_loss: 78.6213 - val_mae: 6.5749\n",
            "Epoch 305/1024\n",
            "404/404 [==============================] - 0s 114us/sample - loss: 62.0034 - mae: 5.7381 - val_loss: 79.7990 - val_mae: 6.6412\n",
            "Epoch 306/1024\n",
            "404/404 [==============================] - 0s 107us/sample - loss: 57.0981 - mae: 5.5157 - val_loss: 81.3383 - val_mae: 6.7314\n",
            "Epoch 307/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 75.8356 - mae: 6.2752 - val_loss: 80.1459 - val_mae: 6.6670\n",
            "Epoch 308/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 62.1138 - mae: 5.6336 - val_loss: 77.6429 - val_mae: 6.5323\n",
            "Epoch 309/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 66.9227 - mae: 6.0607 - val_loss: 74.5752 - val_mae: 6.3716\n",
            "Epoch 310/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 62.8232 - mae: 5.9456 - val_loss: 73.6547 - val_mae: 6.3259\n",
            "Epoch 311/1024\n",
            "404/404 [==============================] - 0s 125us/sample - loss: 62.9114 - mae: 5.8155 - val_loss: 75.4014 - val_mae: 6.4211\n",
            "Epoch 312/1024\n",
            "404/404 [==============================] - 0s 107us/sample - loss: 59.2179 - mae: 5.6163 - val_loss: 79.4260 - val_mae: 6.6403\n",
            "Epoch 313/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 61.0904 - mae: 5.5534 - val_loss: 80.5569 - val_mae: 6.7040\n",
            "Epoch 314/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 62.6035 - mae: 5.6793 - val_loss: 79.1998 - val_mae: 6.6282\n",
            "Epoch 315/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 59.8579 - mae: 5.5495 - val_loss: 76.2441 - val_mae: 6.4669\n",
            "Epoch 316/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 65.3016 - mae: 6.0119 - val_loss: 72.1431 - val_mae: 6.2418\n",
            "Epoch 317/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 61.4319 - mae: 5.6417 - val_loss: 68.6492 - val_mae: 6.0518\n",
            "Epoch 318/1024\n",
            "404/404 [==============================] - 0s 87us/sample - loss: 62.6121 - mae: 5.7380 - val_loss: 68.4583 - val_mae: 6.0392\n",
            "Epoch 319/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 66.2875 - mae: 5.8263 - val_loss: 72.4088 - val_mae: 6.2544\n",
            "Epoch 320/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 61.1205 - mae: 5.8028 - val_loss: 78.7134 - val_mae: 6.6020\n",
            "Epoch 321/1024\n",
            "404/404 [==============================] - 0s 109us/sample - loss: 63.3111 - mae: 5.8109 - val_loss: 83.8747 - val_mae: 6.8747\n",
            "Epoch 322/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 58.7371 - mae: 5.4791 - val_loss: 86.5083 - val_mae: 7.0091\n",
            "Epoch 323/1024\n",
            "404/404 [==============================] - 0s 109us/sample - loss: 63.6772 - mae: 5.6485 - val_loss: 84.4238 - val_mae: 6.8915\n",
            "Epoch 324/1024\n",
            "404/404 [==============================] - 0s 107us/sample - loss: 60.2716 - mae: 5.5982 - val_loss: 79.4986 - val_mae: 6.6311\n",
            "Epoch 325/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 56.5181 - mae: 5.4704 - val_loss: 74.4103 - val_mae: 6.3479\n",
            "Epoch 326/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 56.4760 - mae: 5.5085 - val_loss: 69.3959 - val_mae: 6.0743\n",
            "Epoch 327/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 55.9471 - mae: 5.6447 - val_loss: 66.5209 - val_mae: 5.9140\n",
            "Epoch 328/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 63.0845 - mae: 5.7027 - val_loss: 67.2287 - val_mae: 5.9542\n",
            "Epoch 329/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 62.4742 - mae: 5.7472 - val_loss: 70.2029 - val_mae: 6.1220\n",
            "Epoch 330/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 58.7419 - mae: 5.6724 - val_loss: 75.2845 - val_mae: 6.4105\n",
            "Epoch 331/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 59.3200 - mae: 5.6914 - val_loss: 77.2990 - val_mae: 6.5370\n",
            "Epoch 332/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 69.0466 - mae: 5.9903 - val_loss: 77.1672 - val_mae: 6.5364\n",
            "Epoch 333/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 63.3622 - mae: 5.6541 - val_loss: 72.5779 - val_mae: 6.2689\n",
            "Epoch 334/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 58.9957 - mae: 5.7518 - val_loss: 70.2617 - val_mae: 6.1361\n",
            "Epoch 335/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 55.2681 - mae: 5.5602 - val_loss: 68.4610 - val_mae: 6.0359\n",
            "Epoch 336/1024\n",
            "404/404 [==============================] - 0s 107us/sample - loss: 60.3989 - mae: 5.8797 - val_loss: 69.7736 - val_mae: 6.1142\n",
            "Epoch 337/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 61.3625 - mae: 5.8012 - val_loss: 73.1263 - val_mae: 6.3136\n",
            "Epoch 338/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 59.9470 - mae: 5.5552 - val_loss: 75.9752 - val_mae: 6.4804\n",
            "Epoch 339/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 56.7903 - mae: 5.5124 - val_loss: 76.5038 - val_mae: 6.5088\n",
            "Epoch 340/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 60.7853 - mae: 5.5843 - val_loss: 76.0995 - val_mae: 6.4828\n",
            "Epoch 341/1024\n",
            "404/404 [==============================] - 0s 87us/sample - loss: 65.3379 - mae: 5.8040 - val_loss: 71.8050 - val_mae: 6.2381\n",
            "Epoch 342/1024\n",
            "404/404 [==============================] - 0s 105us/sample - loss: 59.1287 - mae: 5.6414 - val_loss: 68.3616 - val_mae: 6.0424\n",
            "Epoch 343/1024\n",
            "404/404 [==============================] - 0s 88us/sample - loss: 53.8568 - mae: 5.4741 - val_loss: 67.1777 - val_mae: 5.9743\n",
            "Epoch 344/1024\n",
            "404/404 [==============================] - 0s 121us/sample - loss: 57.6837 - mae: 5.4326 - val_loss: 67.7946 - val_mae: 6.0108\n",
            "Epoch 345/1024\n",
            "404/404 [==============================] - 0s 105us/sample - loss: 63.5316 - mae: 5.6099 - val_loss: 70.3496 - val_mae: 6.1603\n",
            "Epoch 346/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 60.8229 - mae: 5.6699 - val_loss: 72.6116 - val_mae: 6.2888\n",
            "Epoch 347/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 53.4558 - mae: 5.3762 - val_loss: 73.0231 - val_mae: 6.3126\n",
            "Epoch 348/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 56.9382 - mae: 5.3788 - val_loss: 70.0918 - val_mae: 6.1491\n",
            "Epoch 349/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 57.8933 - mae: 5.4673 - val_loss: 64.4893 - val_mae: 5.8175\n",
            "Epoch 350/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 55.9118 - mae: 5.4447 - val_loss: 61.1261 - val_mae: 5.6179\n",
            "Epoch 351/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 54.2233 - mae: 5.5396 - val_loss: 63.0713 - val_mae: 5.7315\n",
            "Epoch 352/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 56.5672 - mae: 5.4018 - val_loss: 67.5330 - val_mae: 6.0039\n",
            "Epoch 353/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 59.9663 - mae: 5.7954 - val_loss: 72.3052 - val_mae: 6.3004\n",
            "Epoch 354/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 56.1290 - mae: 5.2736 - val_loss: 71.9471 - val_mae: 6.2797\n",
            "Epoch 355/1024\n",
            "404/404 [==============================] - 0s 112us/sample - loss: 58.4190 - mae: 5.5167 - val_loss: 64.6263 - val_mae: 5.8371\n",
            "Epoch 356/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 58.0831 - mae: 5.4755 - val_loss: 59.3995 - val_mae: 5.5285\n",
            "Epoch 357/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 59.6673 - mae: 5.7769 - val_loss: 61.0454 - val_mae: 5.6173\n",
            "Epoch 358/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 49.7415 - mae: 5.3971 - val_loss: 64.5714 - val_mae: 5.8291\n",
            "Epoch 359/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 57.0654 - mae: 5.5195 - val_loss: 70.1411 - val_mae: 6.1695\n",
            "Epoch 360/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 59.5193 - mae: 5.6929 - val_loss: 75.6839 - val_mae: 6.5176\n",
            "Epoch 361/1024\n",
            "404/404 [==============================] - 0s 110us/sample - loss: 58.3245 - mae: 5.5965 - val_loss: 75.1573 - val_mae: 6.4839\n",
            "Epoch 362/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 68.7752 - mae: 5.9405 - val_loss: 70.8230 - val_mae: 6.2130\n",
            "Epoch 363/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 52.6014 - mae: 5.3643 - val_loss: 62.7211 - val_mae: 5.7080\n",
            "Epoch 364/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 50.5029 - mae: 5.1768 - val_loss: 57.1808 - val_mae: 5.3808\n",
            "Epoch 365/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 51.7199 - mae: 5.4379 - val_loss: 55.6563 - val_mae: 5.2854\n",
            "Epoch 366/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 54.8457 - mae: 5.4708 - val_loss: 57.6695 - val_mae: 5.4058\n",
            "Epoch 367/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 57.5549 - mae: 5.6869 - val_loss: 62.5940 - val_mae: 5.7100\n",
            "Epoch 368/1024\n",
            "404/404 [==============================] - 0s 106us/sample - loss: 59.1390 - mae: 5.6572 - val_loss: 68.7408 - val_mae: 6.1028\n",
            "Epoch 369/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 58.8323 - mae: 5.5523 - val_loss: 72.2806 - val_mae: 6.3256\n",
            "Epoch 370/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 67.7682 - mae: 5.9389 - val_loss: 70.3363 - val_mae: 6.2114\n",
            "Epoch 371/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 56.9995 - mae: 5.5850 - val_loss: 65.0969 - val_mae: 5.8817\n",
            "Epoch 372/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 59.0200 - mae: 5.5963 - val_loss: 61.2088 - val_mae: 5.6302\n",
            "Epoch 373/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 53.3503 - mae: 5.4316 - val_loss: 58.6491 - val_mae: 5.4705\n",
            "Epoch 374/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 52.7046 - mae: 5.2186 - val_loss: 58.5067 - val_mae: 5.4643\n",
            "Epoch 375/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 53.6839 - mae: 5.4639 - val_loss: 60.8905 - val_mae: 5.6226\n",
            "Epoch 376/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 54.6185 - mae: 5.3483 - val_loss: 62.6333 - val_mae: 5.7423\n",
            "Epoch 377/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 56.6030 - mae: 5.4597 - val_loss: 61.8036 - val_mae: 5.6922\n",
            "Epoch 378/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 55.6473 - mae: 5.5566 - val_loss: 61.4601 - val_mae: 5.6714\n",
            "Epoch 379/1024\n",
            "404/404 [==============================] - 0s 108us/sample - loss: 51.7511 - mae: 5.3377 - val_loss: 61.5048 - val_mae: 5.6786\n",
            "Epoch 380/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 50.8070 - mae: 5.3328 - val_loss: 63.7238 - val_mae: 5.8223\n",
            "Epoch 381/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 57.4366 - mae: 5.7257 - val_loss: 62.7519 - val_mae: 5.7650\n",
            "Epoch 382/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 52.0365 - mae: 5.1970 - val_loss: 62.6721 - val_mae: 5.7631\n",
            "Epoch 383/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 55.0138 - mae: 5.4200 - val_loss: 63.0875 - val_mae: 5.7886\n",
            "Epoch 384/1024\n",
            "404/404 [==============================] - 0s 90us/sample - loss: 53.7798 - mae: 5.2713 - val_loss: 61.8719 - val_mae: 5.7113\n",
            "Epoch 385/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 60.0509 - mae: 5.6051 - val_loss: 61.0664 - val_mae: 5.6629\n",
            "Epoch 386/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 55.0729 - mae: 5.4023 - val_loss: 57.5809 - val_mae: 5.4461\n",
            "Epoch 387/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 57.0636 - mae: 5.5272 - val_loss: 57.7231 - val_mae: 5.4529\n",
            "Epoch 388/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 54.5216 - mae: 5.3870 - val_loss: 61.5064 - val_mae: 5.6901\n",
            "Epoch 389/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 50.8164 - mae: 5.3199 - val_loss: 64.5871 - val_mae: 5.8873\n",
            "Epoch 390/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 55.6127 - mae: 5.5307 - val_loss: 65.5656 - val_mae: 5.9499\n",
            "Epoch 391/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 54.6828 - mae: 5.5407 - val_loss: 64.3842 - val_mae: 5.8728\n",
            "Epoch 392/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 49.0991 - mae: 5.1474 - val_loss: 62.3878 - val_mae: 5.7431\n",
            "Epoch 393/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 50.3542 - mae: 5.2450 - val_loss: 60.5734 - val_mae: 5.6193\n",
            "Epoch 394/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 49.4105 - mae: 5.2673 - val_loss: 59.3708 - val_mae: 5.5430\n",
            "Epoch 395/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 52.8172 - mae: 5.4511 - val_loss: 58.2952 - val_mae: 5.4792\n",
            "Epoch 396/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 53.7917 - mae: 5.2986 - val_loss: 54.4524 - val_mae: 5.2348\n",
            "Epoch 397/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 49.6094 - mae: 5.2421 - val_loss: 53.7199 - val_mae: 5.1954\n",
            "Epoch 398/1024\n",
            "404/404 [==============================] - 0s 117us/sample - loss: 57.0588 - mae: 5.6195 - val_loss: 56.1403 - val_mae: 5.3691\n",
            "Epoch 399/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 53.8133 - mae: 5.6179 - val_loss: 62.0143 - val_mae: 5.7585\n",
            "Epoch 400/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 55.3974 - mae: 5.4338 - val_loss: 67.4123 - val_mae: 6.1275\n",
            "Epoch 401/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 60.7175 - mae: 5.6327 - val_loss: 68.4445 - val_mae: 6.2058\n",
            "Epoch 402/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 48.1136 - mae: 5.0162 - val_loss: 65.2696 - val_mae: 6.0040\n",
            "Epoch 403/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 51.2928 - mae: 5.2395 - val_loss: 58.7437 - val_mae: 5.5629\n",
            "Epoch 404/1024\n",
            "404/404 [==============================] - 0s 110us/sample - loss: 50.3601 - mae: 5.4514 - val_loss: 56.1349 - val_mae: 5.3851\n",
            "Epoch 405/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 45.3904 - mae: 4.9943 - val_loss: 54.3809 - val_mae: 5.2707\n",
            "Epoch 406/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 55.7204 - mae: 5.5062 - val_loss: 55.4581 - val_mae: 5.3469\n",
            "Epoch 407/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 48.3421 - mae: 5.2955 - val_loss: 61.1228 - val_mae: 5.7425\n",
            "Epoch 408/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 51.0869 - mae: 5.1593 - val_loss: 68.9713 - val_mae: 6.2649\n",
            "Epoch 409/1024\n",
            "404/404 [==============================] - 0s 90us/sample - loss: 50.8196 - mae: 5.2345 - val_loss: 70.7370 - val_mae: 6.3831\n",
            "Epoch 410/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 56.3176 - mae: 5.5560 - val_loss: 67.9514 - val_mae: 6.1974\n",
            "Epoch 411/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 55.4961 - mae: 5.4167 - val_loss: 58.6166 - val_mae: 5.5713\n",
            "Epoch 412/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 48.2227 - mae: 5.2097 - val_loss: 53.3312 - val_mae: 5.2118\n",
            "Epoch 413/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 54.5681 - mae: 5.3427 - val_loss: 51.9572 - val_mae: 5.1182\n",
            "Epoch 414/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 53.2755 - mae: 5.3778 - val_loss: 55.5079 - val_mae: 5.3594\n",
            "Epoch 415/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 45.9650 - mae: 5.0025 - val_loss: 60.5296 - val_mae: 5.6896\n",
            "Epoch 416/1024\n",
            "404/404 [==============================] - 0s 106us/sample - loss: 48.0511 - mae: 5.2199 - val_loss: 63.8524 - val_mae: 5.9039\n",
            "Epoch 417/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 51.1720 - mae: 5.2307 - val_loss: 66.3731 - val_mae: 6.0810\n",
            "Epoch 418/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 52.5190 - mae: 5.3576 - val_loss: 63.8655 - val_mae: 5.9144\n",
            "Epoch 419/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 60.7626 - mae: 5.6743 - val_loss: 58.7486 - val_mae: 5.5833\n",
            "Epoch 420/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 55.4717 - mae: 5.3553 - val_loss: 52.4840 - val_mae: 5.1661\n",
            "Epoch 421/1024\n",
            "404/404 [==============================] - 0s 86us/sample - loss: 50.9505 - mae: 5.0091 - val_loss: 49.6645 - val_mae: 4.9742\n",
            "Epoch 422/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 49.9445 - mae: 5.4250 - val_loss: 52.8713 - val_mae: 5.1981\n",
            "Epoch 423/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 49.9027 - mae: 5.3545 - val_loss: 56.4165 - val_mae: 5.4457\n",
            "Epoch 424/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 48.8615 - mae: 5.2533 - val_loss: 59.9668 - val_mae: 5.6983\n",
            "Epoch 425/1024\n",
            "404/404 [==============================] - 0s 106us/sample - loss: 50.4279 - mae: 5.2176 - val_loss: 59.4108 - val_mae: 5.6685\n",
            "Epoch 426/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 47.2027 - mae: 5.0165 - val_loss: 59.4716 - val_mae: 5.6837\n",
            "Epoch 427/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 50.1194 - mae: 5.2701 - val_loss: 62.0010 - val_mae: 5.8571\n",
            "Epoch 428/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 52.4414 - mae: 5.3311 - val_loss: 64.6965 - val_mae: 6.0326\n",
            "Epoch 429/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 47.7604 - mae: 5.2015 - val_loss: 63.8757 - val_mae: 5.9761\n",
            "Epoch 430/1024\n",
            "404/404 [==============================] - 0s 90us/sample - loss: 51.6408 - mae: 5.2975 - val_loss: 60.4615 - val_mae: 5.7531\n",
            "Epoch 431/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 44.9794 - mae: 5.0171 - val_loss: 56.5722 - val_mae: 5.4871\n",
            "Epoch 432/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 52.5460 - mae: 5.2777 - val_loss: 53.0965 - val_mae: 5.2432\n",
            "Epoch 433/1024\n",
            "404/404 [==============================] - 0s 90us/sample - loss: 45.8247 - mae: 4.9680 - val_loss: 50.1394 - val_mae: 5.0311\n",
            "Epoch 434/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 44.0055 - mae: 4.9539 - val_loss: 51.0945 - val_mae: 5.1097\n",
            "Epoch 435/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 51.2474 - mae: 5.4081 - val_loss: 54.2636 - val_mae: 5.3538\n",
            "Epoch 436/1024\n",
            "404/404 [==============================] - 0s 89us/sample - loss: 49.8349 - mae: 5.3078 - val_loss: 58.6062 - val_mae: 5.6658\n",
            "Epoch 437/1024\n",
            "404/404 [==============================] - 0s 88us/sample - loss: 47.8946 - mae: 4.9878 - val_loss: 60.6839 - val_mae: 5.8140\n",
            "Epoch 438/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 47.0124 - mae: 5.0899 - val_loss: 61.2518 - val_mae: 5.8606\n",
            "Epoch 439/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 50.9915 - mae: 5.2898 - val_loss: 56.6611 - val_mae: 5.5577\n",
            "Epoch 440/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 47.8876 - mae: 5.1560 - val_loss: 52.5438 - val_mae: 5.2811\n",
            "Epoch 441/1024\n",
            "404/404 [==============================] - 0s 109us/sample - loss: 42.4071 - mae: 4.7252 - val_loss: 48.3265 - val_mae: 4.9692\n",
            "Epoch 442/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 51.9956 - mae: 5.3919 - val_loss: 46.2939 - val_mae: 4.8117\n",
            "Epoch 443/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 50.9716 - mae: 5.4533 - val_loss: 49.7219 - val_mae: 5.0626\n",
            "Epoch 444/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 48.6430 - mae: 5.0710 - val_loss: 56.2196 - val_mae: 5.5304\n",
            "Epoch 445/1024\n",
            "404/404 [==============================] - 0s 114us/sample - loss: 51.2873 - mae: 5.1280 - val_loss: 60.3878 - val_mae: 5.8110\n",
            "Epoch 446/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 50.9259 - mae: 5.3063 - val_loss: 57.9715 - val_mae: 5.6433\n",
            "Epoch 447/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 43.8182 - mae: 5.0159 - val_loss: 53.3192 - val_mae: 5.3106\n",
            "Epoch 448/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 43.7277 - mae: 4.9274 - val_loss: 48.4589 - val_mae: 4.9441\n",
            "Epoch 449/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 48.2546 - mae: 5.1981 - val_loss: 50.0554 - val_mae: 5.0679\n",
            "Epoch 450/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 47.8496 - mae: 5.1957 - val_loss: 54.1987 - val_mae: 5.3801\n",
            "Epoch 451/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 44.2192 - mae: 4.9871 - val_loss: 57.2869 - val_mae: 5.6018\n",
            "Epoch 452/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 48.0684 - mae: 5.1478 - val_loss: 58.0663 - val_mae: 5.6535\n",
            "Epoch 453/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 47.3497 - mae: 5.0747 - val_loss: 55.8029 - val_mae: 5.4909\n",
            "Epoch 454/1024\n",
            "404/404 [==============================] - 0s 128us/sample - loss: 52.0189 - mae: 5.1479 - val_loss: 52.6752 - val_mae: 5.2599\n",
            "Epoch 455/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 43.5316 - mae: 4.9789 - val_loss: 50.3930 - val_mae: 5.0905\n",
            "Epoch 456/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 45.0296 - mae: 5.1283 - val_loss: 49.1338 - val_mae: 5.0010\n",
            "Epoch 457/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 47.4871 - mae: 5.1196 - val_loss: 49.2996 - val_mae: 5.0243\n",
            "Epoch 458/1024\n",
            "404/404 [==============================] - 0s 107us/sample - loss: 48.0155 - mae: 5.0793 - val_loss: 49.1241 - val_mae: 5.0271\n",
            "Epoch 459/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 47.3600 - mae: 5.1369 - val_loss: 49.6683 - val_mae: 5.0821\n",
            "Epoch 460/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 46.8817 - mae: 4.9879 - val_loss: 51.9928 - val_mae: 5.2683\n",
            "Epoch 461/1024\n",
            "404/404 [==============================] - 0s 124us/sample - loss: 42.1218 - mae: 4.8409 - val_loss: 53.3603 - val_mae: 5.3859\n",
            "Epoch 462/1024\n",
            "404/404 [==============================] - 0s 137us/sample - loss: 43.8281 - mae: 4.8024 - val_loss: 55.3571 - val_mae: 5.5407\n",
            "Epoch 463/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 41.8183 - mae: 4.7946 - val_loss: 55.7793 - val_mae: 5.5783\n",
            "Epoch 464/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 45.6429 - mae: 5.0289 - val_loss: 51.9007 - val_mae: 5.2954\n",
            "Epoch 465/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 50.1889 - mae: 5.4239 - val_loss: 50.9989 - val_mae: 5.2318\n",
            "Epoch 466/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 50.2660 - mae: 5.3579 - val_loss: 51.9707 - val_mae: 5.3001\n",
            "Epoch 467/1024\n",
            "404/404 [==============================] - 0s 90us/sample - loss: 44.1628 - mae: 5.0841 - val_loss: 52.2699 - val_mae: 5.3152\n",
            "Epoch 468/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 43.0867 - mae: 5.0601 - val_loss: 55.5912 - val_mae: 5.5535\n",
            "Epoch 469/1024\n",
            "404/404 [==============================] - 0s 107us/sample - loss: 52.3726 - mae: 5.2888 - val_loss: 58.8186 - val_mae: 5.7858\n",
            "Epoch 470/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 46.7368 - mae: 5.1501 - val_loss: 58.8747 - val_mae: 5.7828\n",
            "Epoch 471/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 47.7499 - mae: 5.0539 - val_loss: 55.9987 - val_mae: 5.5651\n",
            "Epoch 472/1024\n",
            "404/404 [==============================] - 0s 141us/sample - loss: 39.1272 - mae: 4.8144 - val_loss: 53.4392 - val_mae: 5.3787\n",
            "Epoch 473/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 46.5975 - mae: 5.0545 - val_loss: 51.0181 - val_mae: 5.2110\n",
            "Epoch 474/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 45.6581 - mae: 5.0990 - val_loss: 48.1507 - val_mae: 5.0097\n",
            "Epoch 475/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 44.7581 - mae: 5.2453 - val_loss: 50.4899 - val_mae: 5.1991\n",
            "Epoch 476/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 39.7115 - mae: 4.6873 - val_loss: 54.2504 - val_mae: 5.4761\n",
            "Epoch 477/1024\n",
            "404/404 [==============================] - 0s 88us/sample - loss: 46.3371 - mae: 5.1955 - val_loss: 59.0797 - val_mae: 5.8184\n",
            "Epoch 478/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 43.7129 - mae: 4.9919 - val_loss: 62.6169 - val_mae: 6.0665\n",
            "Epoch 479/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 44.7257 - mae: 4.9870 - val_loss: 58.9301 - val_mae: 5.8024\n",
            "Epoch 480/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 48.1430 - mae: 5.0796 - val_loss: 49.7635 - val_mae: 5.1434\n",
            "Epoch 481/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 42.6452 - mae: 4.9580 - val_loss: 43.3767 - val_mae: 4.6541\n",
            "Epoch 482/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 43.5348 - mae: 4.9063 - val_loss: 42.0312 - val_mae: 4.5574\n",
            "Epoch 483/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 46.2172 - mae: 5.2383 - val_loss: 45.4939 - val_mae: 4.8235\n",
            "Epoch 484/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 49.6337 - mae: 5.1871 - val_loss: 55.3727 - val_mae: 5.5626\n",
            "Epoch 485/1024\n",
            "404/404 [==============================] - 0s 87us/sample - loss: 52.2730 - mae: 5.3606 - val_loss: 60.4700 - val_mae: 5.9198\n",
            "Epoch 486/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 44.3713 - mae: 4.9528 - val_loss: 59.0308 - val_mae: 5.8232\n",
            "Epoch 487/1024\n",
            "404/404 [==============================] - 0s 108us/sample - loss: 47.2670 - mae: 5.2142 - val_loss: 53.1133 - val_mae: 5.4094\n",
            "Epoch 488/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 45.4387 - mae: 5.1728 - val_loss: 48.1367 - val_mae: 5.0433\n",
            "Epoch 489/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 47.1342 - mae: 5.1367 - val_loss: 44.7988 - val_mae: 4.7887\n",
            "Epoch 490/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 43.7890 - mae: 5.1574 - val_loss: 46.3246 - val_mae: 4.9170\n",
            "Epoch 491/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 46.1960 - mae: 5.1593 - val_loss: 52.3886 - val_mae: 5.3714\n",
            "Epoch 492/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 44.3513 - mae: 4.9923 - val_loss: 62.1167 - val_mae: 6.0661\n",
            "Epoch 493/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 42.4086 - mae: 5.0222 - val_loss: 64.1352 - val_mae: 6.2084\n",
            "Epoch 494/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 41.9907 - mae: 4.9833 - val_loss: 58.5397 - val_mae: 5.8089\n",
            "Epoch 495/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 45.5839 - mae: 4.9920 - val_loss: 49.0690 - val_mae: 5.1258\n",
            "Epoch 496/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 40.7956 - mae: 4.7877 - val_loss: 45.3238 - val_mae: 4.8208\n",
            "Epoch 497/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 45.7942 - mae: 5.1014 - val_loss: 46.1763 - val_mae: 4.8955\n",
            "Epoch 498/1024\n",
            "404/404 [==============================] - 0s 110us/sample - loss: 48.1333 - mae: 5.1334 - val_loss: 50.0808 - val_mae: 5.1991\n",
            "Epoch 499/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 44.7205 - mae: 5.1560 - val_loss: 52.9669 - val_mae: 5.4088\n",
            "Epoch 500/1024\n",
            "404/404 [==============================] - 0s 89us/sample - loss: 45.6873 - mae: 4.9017 - val_loss: 54.9914 - val_mae: 5.5503\n",
            "Epoch 501/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 42.5415 - mae: 4.9343 - val_loss: 55.4339 - val_mae: 5.5791\n",
            "Epoch 502/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 50.0172 - mae: 5.4276 - val_loss: 54.5176 - val_mae: 5.5104\n",
            "Epoch 503/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 42.3524 - mae: 4.8502 - val_loss: 52.9912 - val_mae: 5.3978\n",
            "Epoch 504/1024\n",
            "404/404 [==============================] - 0s 129us/sample - loss: 41.5888 - mae: 5.0124 - val_loss: 50.8946 - val_mae: 5.2433\n",
            "Epoch 505/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 45.9408 - mae: 5.1095 - val_loss: 49.5372 - val_mae: 5.1482\n",
            "Epoch 506/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 39.9632 - mae: 4.7794 - val_loss: 50.5263 - val_mae: 5.2321\n",
            "Epoch 507/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 45.4473 - mae: 5.2096 - val_loss: 54.4630 - val_mae: 5.5209\n",
            "Epoch 508/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 42.8319 - mae: 4.8721 - val_loss: 54.3143 - val_mae: 5.5149\n",
            "Epoch 509/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 45.8533 - mae: 5.2731 - val_loss: 50.7573 - val_mae: 5.2641\n",
            "Epoch 510/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 44.2003 - mae: 5.0697 - val_loss: 47.8800 - val_mae: 5.0479\n",
            "Epoch 511/1024\n",
            "404/404 [==============================] - 0s 114us/sample - loss: 46.5259 - mae: 5.1688 - val_loss: 47.9471 - val_mae: 5.0585\n",
            "Epoch 512/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 37.6499 - mae: 4.7185 - val_loss: 52.3311 - val_mae: 5.3905\n",
            "Epoch 513/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 43.7767 - mae: 4.9760 - val_loss: 55.0184 - val_mae: 5.5879\n",
            "Epoch 514/1024\n",
            "404/404 [==============================] - 0s 114us/sample - loss: 41.3475 - mae: 4.8556 - val_loss: 53.6811 - val_mae: 5.4956\n",
            "Epoch 515/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 43.5401 - mae: 4.9783 - val_loss: 53.3637 - val_mae: 5.4700\n",
            "Epoch 516/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 47.0864 - mae: 5.0251 - val_loss: 51.4206 - val_mae: 5.3272\n",
            "Epoch 517/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 46.0669 - mae: 4.9781 - val_loss: 50.4048 - val_mae: 5.2485\n",
            "Epoch 518/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 44.4819 - mae: 4.8682 - val_loss: 46.9119 - val_mae: 4.9698\n",
            "Epoch 519/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 50.0359 - mae: 5.2794 - val_loss: 47.9094 - val_mae: 5.0448\n",
            "Epoch 520/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 40.9471 - mae: 4.9348 - val_loss: 49.6028 - val_mae: 5.1805\n",
            "Epoch 521/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 39.7283 - mae: 4.6683 - val_loss: 51.0111 - val_mae: 5.2906\n",
            "Epoch 522/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 43.0940 - mae: 5.0237 - val_loss: 51.6471 - val_mae: 5.3421\n",
            "Epoch 523/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 46.5171 - mae: 5.0550 - val_loss: 52.6549 - val_mae: 5.4186\n",
            "Epoch 524/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 47.4043 - mae: 5.1533 - val_loss: 53.3132 - val_mae: 5.4742\n",
            "Epoch 525/1024\n",
            "404/404 [==============================] - 0s 88us/sample - loss: 43.2879 - mae: 4.8582 - val_loss: 48.7570 - val_mae: 5.1421\n",
            "Epoch 526/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 43.4874 - mae: 4.9534 - val_loss: 45.2024 - val_mae: 4.8542\n",
            "Epoch 527/1024\n",
            "404/404 [==============================] - 0s 117us/sample - loss: 41.8940 - mae: 4.8707 - val_loss: 43.4970 - val_mae: 4.7126\n",
            "Epoch 528/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 42.3839 - mae: 4.8305 - val_loss: 44.4842 - val_mae: 4.7902\n",
            "Epoch 529/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 44.7215 - mae: 5.0905 - val_loss: 49.2753 - val_mae: 5.1679\n",
            "Epoch 530/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 40.4173 - mae: 4.6202 - val_loss: 51.9641 - val_mae: 5.3733\n",
            "Epoch 531/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 47.6150 - mae: 5.2920 - val_loss: 55.4137 - val_mae: 5.6270\n",
            "Epoch 532/1024\n",
            "404/404 [==============================] - 0s 111us/sample - loss: 46.7038 - mae: 5.1020 - val_loss: 51.5854 - val_mae: 5.3507\n",
            "Epoch 533/1024\n",
            "404/404 [==============================] - 0s 109us/sample - loss: 46.1768 - mae: 4.9525 - val_loss: 49.0272 - val_mae: 5.1554\n",
            "Epoch 534/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 42.3854 - mae: 4.8683 - val_loss: 47.9333 - val_mae: 5.0660\n",
            "Epoch 535/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 47.1347 - mae: 5.0638 - val_loss: 49.4027 - val_mae: 5.1763\n",
            "Epoch 536/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 43.6988 - mae: 4.9217 - val_loss: 48.5615 - val_mae: 5.1088\n",
            "Epoch 537/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 47.3761 - mae: 5.2322 - val_loss: 47.2271 - val_mae: 5.0055\n",
            "Epoch 538/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 37.0928 - mae: 4.7218 - val_loss: 47.6133 - val_mae: 5.0373\n",
            "Epoch 539/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 40.9378 - mae: 4.7967 - val_loss: 48.9053 - val_mae: 5.1401\n",
            "Epoch 540/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 39.5555 - mae: 4.7095 - val_loss: 48.6116 - val_mae: 5.1299\n",
            "Epoch 541/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 40.7224 - mae: 4.6937 - val_loss: 48.2858 - val_mae: 5.1202\n",
            "Epoch 542/1024\n",
            "404/404 [==============================] - 0s 86us/sample - loss: 46.2729 - mae: 5.0849 - val_loss: 48.0525 - val_mae: 5.1080\n",
            "Epoch 543/1024\n",
            "404/404 [==============================] - 0s 124us/sample - loss: 37.2811 - mae: 4.5371 - val_loss: 46.1167 - val_mae: 4.9613\n",
            "Epoch 544/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 39.2860 - mae: 4.5897 - val_loss: 43.6621 - val_mae: 4.7695\n",
            "Epoch 545/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 42.5629 - mae: 4.9113 - val_loss: 45.6589 - val_mae: 4.9262\n",
            "Epoch 546/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 41.8911 - mae: 4.9072 - val_loss: 49.4116 - val_mae: 5.2083\n",
            "Epoch 547/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 39.5637 - mae: 4.7907 - val_loss: 51.8870 - val_mae: 5.3866\n",
            "Epoch 548/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 38.9026 - mae: 4.7651 - val_loss: 51.7240 - val_mae: 5.3702\n",
            "Epoch 549/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 40.3558 - mae: 4.7900 - val_loss: 47.9214 - val_mae: 5.0770\n",
            "Epoch 550/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 37.8850 - mae: 4.6902 - val_loss: 40.9373 - val_mae: 4.5303\n",
            "Epoch 551/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 45.4913 - mae: 5.0617 - val_loss: 41.4388 - val_mae: 4.5658\n",
            "Epoch 552/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 42.3362 - mae: 4.8587 - val_loss: 45.7938 - val_mae: 4.9127\n",
            "Epoch 553/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 38.4440 - mae: 4.7481 - val_loss: 48.9759 - val_mae: 5.1655\n",
            "Epoch 554/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 39.5174 - mae: 4.5277 - val_loss: 48.1697 - val_mae: 5.1076\n",
            "Epoch 555/1024\n",
            "404/404 [==============================] - 0s 106us/sample - loss: 36.7355 - mae: 4.6303 - val_loss: 49.1116 - val_mae: 5.1863\n",
            "Epoch 556/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 39.6568 - mae: 4.6980 - val_loss: 47.0602 - val_mae: 5.0313\n",
            "Epoch 557/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 38.0072 - mae: 4.6309 - val_loss: 42.3258 - val_mae: 4.6701\n",
            "Epoch 558/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 42.7009 - mae: 4.9865 - val_loss: 41.8299 - val_mae: 4.6328\n",
            "Epoch 559/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 42.5242 - mae: 5.0554 - val_loss: 45.0885 - val_mae: 4.8787\n",
            "Epoch 560/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 38.5615 - mae: 4.7822 - val_loss: 49.2145 - val_mae: 5.2005\n",
            "Epoch 561/1024\n",
            "404/404 [==============================] - 0s 114us/sample - loss: 41.1950 - mae: 4.7441 - val_loss: 53.3667 - val_mae: 5.5135\n",
            "Epoch 562/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 47.2182 - mae: 4.8949 - val_loss: 53.0136 - val_mae: 5.4867\n",
            "Epoch 563/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 40.5354 - mae: 4.6412 - val_loss: 48.2774 - val_mae: 5.1253\n",
            "Epoch 564/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 44.4561 - mae: 5.1218 - val_loss: 41.3109 - val_mae: 4.5955\n",
            "Epoch 565/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 44.7134 - mae: 4.9774 - val_loss: 40.9889 - val_mae: 4.5750\n",
            "Epoch 566/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 41.7810 - mae: 4.6839 - val_loss: 44.3615 - val_mae: 4.8131\n",
            "Epoch 567/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 46.1580 - mae: 4.9458 - val_loss: 46.9353 - val_mae: 5.0082\n",
            "Epoch 568/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 40.0474 - mae: 4.6254 - val_loss: 50.7382 - val_mae: 5.2904\n",
            "Epoch 569/1024\n",
            "404/404 [==============================] - 0s 86us/sample - loss: 34.5780 - mae: 4.3590 - val_loss: 52.6172 - val_mae: 5.4267\n",
            "Epoch 570/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 43.2192 - mae: 4.9655 - val_loss: 48.4822 - val_mae: 5.1144\n",
            "Epoch 571/1024\n",
            "404/404 [==============================] - 0s 105us/sample - loss: 40.9693 - mae: 4.6749 - val_loss: 42.5754 - val_mae: 4.6619\n",
            "Epoch 572/1024\n",
            "404/404 [==============================] - 0s 118us/sample - loss: 38.8039 - mae: 4.7386 - val_loss: 40.2196 - val_mae: 4.4888\n",
            "Epoch 573/1024\n",
            "404/404 [==============================] - 0s 110us/sample - loss: 41.8291 - mae: 4.9955 - val_loss: 41.5949 - val_mae: 4.5868\n",
            "Epoch 574/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 39.0018 - mae: 4.6514 - val_loss: 44.6251 - val_mae: 4.8202\n",
            "Epoch 575/1024\n",
            "404/404 [==============================] - 0s 107us/sample - loss: 38.4094 - mae: 4.7742 - val_loss: 48.3254 - val_mae: 5.1114\n",
            "Epoch 576/1024\n",
            "404/404 [==============================] - 0s 108us/sample - loss: 41.2933 - mae: 4.7057 - val_loss: 51.4573 - val_mae: 5.3601\n",
            "Epoch 577/1024\n",
            "404/404 [==============================] - 0s 105us/sample - loss: 38.7329 - mae: 4.6711 - val_loss: 49.5897 - val_mae: 5.2252\n",
            "Epoch 578/1024\n",
            "404/404 [==============================] - 0s 107us/sample - loss: 46.5593 - mae: 5.0419 - val_loss: 45.2568 - val_mae: 4.8877\n",
            "Epoch 579/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 39.5670 - mae: 4.7155 - val_loss: 41.6283 - val_mae: 4.6202\n",
            "Epoch 580/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 40.9410 - mae: 4.8377 - val_loss: 40.0634 - val_mae: 4.5098\n",
            "Epoch 581/1024\n",
            "404/404 [==============================] - 0s 105us/sample - loss: 38.5895 - mae: 4.5512 - val_loss: 42.8661 - val_mae: 4.7051\n",
            "Epoch 582/1024\n",
            "404/404 [==============================] - 0s 117us/sample - loss: 30.8690 - mae: 4.2146 - val_loss: 46.3284 - val_mae: 4.9749\n",
            "Epoch 583/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 39.4821 - mae: 4.7791 - val_loss: 48.1282 - val_mae: 5.1152\n",
            "Epoch 584/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 42.9392 - mae: 4.7366 - val_loss: 49.0833 - val_mae: 5.1893\n",
            "Epoch 585/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 40.1082 - mae: 4.8023 - val_loss: 48.1776 - val_mae: 5.1241\n",
            "Epoch 586/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 42.1290 - mae: 4.8983 - val_loss: 45.0645 - val_mae: 4.8820\n",
            "Epoch 587/1024\n",
            "404/404 [==============================] - 0s 90us/sample - loss: 41.5019 - mae: 4.6590 - val_loss: 40.9139 - val_mae: 4.5812\n",
            "Epoch 588/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 42.2671 - mae: 4.7397 - val_loss: 39.6010 - val_mae: 4.4955\n",
            "Epoch 589/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 39.6588 - mae: 4.7797 - val_loss: 41.6315 - val_mae: 4.6356\n",
            "Epoch 590/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 42.7390 - mae: 4.8247 - val_loss: 48.2926 - val_mae: 5.1168\n",
            "Epoch 591/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 39.5034 - mae: 4.6885 - val_loss: 49.7137 - val_mae: 5.2215\n",
            "Epoch 592/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 41.7621 - mae: 4.7142 - val_loss: 46.0055 - val_mae: 4.9243\n",
            "Epoch 593/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 44.2060 - mae: 4.7998 - val_loss: 40.0405 - val_mae: 4.4802\n",
            "Epoch 594/1024\n",
            "404/404 [==============================] - 0s 114us/sample - loss: 38.4577 - mae: 4.7863 - val_loss: 38.2671 - val_mae: 4.3530\n",
            "Epoch 595/1024\n",
            "404/404 [==============================] - 0s 112us/sample - loss: 40.4413 - mae: 4.8599 - val_loss: 41.4630 - val_mae: 4.5791\n",
            "Epoch 596/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 43.6215 - mae: 4.9417 - val_loss: 47.0317 - val_mae: 4.9994\n",
            "Epoch 597/1024\n",
            "404/404 [==============================] - 0s 107us/sample - loss: 41.6841 - mae: 4.7934 - val_loss: 52.5129 - val_mae: 5.4224\n",
            "Epoch 598/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 44.6933 - mae: 4.9374 - val_loss: 52.6059 - val_mae: 5.4315\n",
            "Epoch 599/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 37.9330 - mae: 4.6180 - val_loss: 49.1235 - val_mae: 5.1755\n",
            "Epoch 600/1024\n",
            "404/404 [==============================] - 0s 87us/sample - loss: 44.8580 - mae: 4.8912 - val_loss: 42.6605 - val_mae: 4.7177\n",
            "Epoch 601/1024\n",
            "404/404 [==============================] - 0s 106us/sample - loss: 40.9049 - mae: 4.8736 - val_loss: 40.3957 - val_mae: 4.5586\n",
            "Epoch 602/1024\n",
            "404/404 [==============================] - 0s 86us/sample - loss: 42.3592 - mae: 4.8790 - val_loss: 43.0103 - val_mae: 4.7077\n",
            "Epoch 603/1024\n",
            "404/404 [==============================] - 0s 108us/sample - loss: 37.3107 - mae: 4.5388 - val_loss: 46.7457 - val_mae: 4.9969\n",
            "Epoch 604/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 40.9832 - mae: 4.7908 - val_loss: 48.5616 - val_mae: 5.1254\n",
            "Epoch 605/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 35.9721 - mae: 4.5353 - val_loss: 45.0150 - val_mae: 4.8395\n",
            "Epoch 606/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 37.2368 - mae: 4.7649 - val_loss: 44.8655 - val_mae: 4.8236\n",
            "Epoch 607/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 36.2730 - mae: 4.5619 - val_loss: 43.6131 - val_mae: 4.7401\n",
            "Epoch 608/1024\n",
            "404/404 [==============================] - 0s 105us/sample - loss: 36.9338 - mae: 4.6006 - val_loss: 40.9957 - val_mae: 4.5638\n",
            "Epoch 609/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 42.2800 - mae: 4.9634 - val_loss: 40.3986 - val_mae: 4.5596\n",
            "Epoch 610/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 38.8704 - mae: 4.6522 - val_loss: 42.6611 - val_mae: 4.7428\n",
            "Epoch 611/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 41.8781 - mae: 4.7427 - val_loss: 45.3596 - val_mae: 4.9582\n",
            "Epoch 612/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 37.3613 - mae: 4.7650 - val_loss: 48.2864 - val_mae: 5.1905\n",
            "Epoch 613/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 39.4356 - mae: 4.7391 - val_loss: 44.8066 - val_mae: 4.9367\n",
            "Epoch 614/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 39.5685 - mae: 4.6704 - val_loss: 40.8545 - val_mae: 4.6634\n",
            "Epoch 615/1024\n",
            "404/404 [==============================] - 0s 123us/sample - loss: 43.9790 - mae: 4.9753 - val_loss: 38.5367 - val_mae: 4.4913\n",
            "Epoch 616/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 40.9321 - mae: 4.7486 - val_loss: 40.0403 - val_mae: 4.5935\n",
            "Epoch 617/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 42.5625 - mae: 4.9456 - val_loss: 46.5609 - val_mae: 5.0494\n",
            "Epoch 618/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 40.3112 - mae: 4.6856 - val_loss: 51.3879 - val_mae: 5.4213\n",
            "Epoch 619/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 34.5039 - mae: 4.6418 - val_loss: 48.7808 - val_mae: 5.2194\n",
            "Epoch 620/1024\n",
            "404/404 [==============================] - 0s 106us/sample - loss: 38.8666 - mae: 4.6530 - val_loss: 41.1865 - val_mae: 4.6238\n",
            "Epoch 621/1024\n",
            "404/404 [==============================] - 0s 105us/sample - loss: 43.1570 - mae: 4.9669 - val_loss: 37.1902 - val_mae: 4.3270\n",
            "Epoch 622/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 43.1925 - mae: 4.7399 - val_loss: 36.1646 - val_mae: 4.2467\n",
            "Epoch 623/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 40.8540 - mae: 4.8441 - val_loss: 38.4842 - val_mae: 4.3881\n",
            "Epoch 624/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 42.6910 - mae: 4.8298 - val_loss: 43.1364 - val_mae: 4.7411\n",
            "Epoch 625/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 38.6840 - mae: 4.6435 - val_loss: 46.9402 - val_mae: 5.0322\n",
            "Epoch 626/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 43.2486 - mae: 4.7598 - val_loss: 47.6082 - val_mae: 5.0738\n",
            "Epoch 627/1024\n",
            "404/404 [==============================] - 0s 107us/sample - loss: 36.8475 - mae: 4.5076 - val_loss: 47.5216 - val_mae: 5.0642\n",
            "Epoch 628/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 39.6491 - mae: 4.7082 - val_loss: 43.4194 - val_mae: 4.7448\n",
            "Epoch 629/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 34.9646 - mae: 4.4328 - val_loss: 39.6933 - val_mae: 4.4746\n",
            "Epoch 630/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 36.8584 - mae: 4.3384 - val_loss: 37.3943 - val_mae: 4.3173\n",
            "Epoch 631/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 33.2733 - mae: 4.3825 - val_loss: 37.9142 - val_mae: 4.3696\n",
            "Epoch 632/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 38.9895 - mae: 4.5268 - val_loss: 40.9223 - val_mae: 4.6118\n",
            "Epoch 633/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 39.2814 - mae: 4.6352 - val_loss: 46.4235 - val_mae: 5.0558\n",
            "Epoch 634/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 37.4371 - mae: 4.5678 - val_loss: 48.3374 - val_mae: 5.2172\n",
            "Epoch 635/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 40.7351 - mae: 4.7410 - val_loss: 45.6505 - val_mae: 5.0043\n",
            "Epoch 636/1024\n",
            "404/404 [==============================] - 0s 110us/sample - loss: 38.8441 - mae: 4.6850 - val_loss: 40.5484 - val_mae: 4.6331\n",
            "Epoch 637/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 40.2081 - mae: 4.7254 - val_loss: 39.5550 - val_mae: 4.5462\n",
            "Epoch 638/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 37.1067 - mae: 4.7102 - val_loss: 38.1631 - val_mae: 4.4382\n",
            "Epoch 639/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 33.3602 - mae: 4.3667 - val_loss: 38.2028 - val_mae: 4.4293\n",
            "Epoch 640/1024\n",
            "404/404 [==============================] - 0s 119us/sample - loss: 40.5006 - mae: 4.7069 - val_loss: 42.1069 - val_mae: 4.6965\n",
            "Epoch 641/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 35.7665 - mae: 4.5914 - val_loss: 44.7302 - val_mae: 4.8824\n",
            "Epoch 642/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 36.1927 - mae: 4.5874 - val_loss: 44.4780 - val_mae: 4.8589\n",
            "Epoch 643/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 37.0103 - mae: 4.5588 - val_loss: 41.7154 - val_mae: 4.6355\n",
            "Epoch 644/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 34.7401 - mae: 4.4731 - val_loss: 41.7843 - val_mae: 4.6405\n",
            "Epoch 645/1024\n",
            "404/404 [==============================] - 0s 89us/sample - loss: 38.1827 - mae: 4.6880 - val_loss: 41.3037 - val_mae: 4.6045\n",
            "Epoch 646/1024\n",
            "404/404 [==============================] - 0s 113us/sample - loss: 38.1524 - mae: 4.4645 - val_loss: 41.1564 - val_mae: 4.5949\n",
            "Epoch 647/1024\n",
            "404/404 [==============================] - 0s 105us/sample - loss: 36.9989 - mae: 4.6248 - val_loss: 42.5404 - val_mae: 4.7075\n",
            "Epoch 648/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 37.3774 - mae: 4.5521 - val_loss: 43.0608 - val_mae: 4.7555\n",
            "Epoch 649/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 39.3930 - mae: 4.7243 - val_loss: 40.8646 - val_mae: 4.5908\n",
            "Epoch 650/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 33.4423 - mae: 4.4370 - val_loss: 39.6242 - val_mae: 4.5009\n",
            "Epoch 651/1024\n",
            "404/404 [==============================] - 0s 111us/sample - loss: 40.8275 - mae: 4.7078 - val_loss: 40.0814 - val_mae: 4.5578\n",
            "Epoch 652/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 43.2776 - mae: 4.8357 - val_loss: 44.4546 - val_mae: 4.8897\n",
            "Epoch 653/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 34.8469 - mae: 4.4486 - val_loss: 47.4247 - val_mae: 5.1331\n",
            "Epoch 654/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 40.8050 - mae: 4.7420 - val_loss: 45.8624 - val_mae: 5.0190\n",
            "Epoch 655/1024\n",
            "404/404 [==============================] - 0s 125us/sample - loss: 30.9791 - mae: 4.1673 - val_loss: 42.5944 - val_mae: 4.8056\n",
            "Epoch 656/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 34.0307 - mae: 4.4383 - val_loss: 40.0378 - val_mae: 4.6185\n",
            "Epoch 657/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 40.5804 - mae: 4.7632 - val_loss: 38.8343 - val_mae: 4.5203\n",
            "Epoch 658/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 37.3123 - mae: 4.5503 - val_loss: 40.1730 - val_mae: 4.6001\n",
            "Epoch 659/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 36.6028 - mae: 4.6825 - val_loss: 45.0769 - val_mae: 4.9225\n",
            "Epoch 660/1024\n",
            "404/404 [==============================] - 0s 108us/sample - loss: 41.3639 - mae: 4.7909 - val_loss: 47.3938 - val_mae: 5.0969\n",
            "Epoch 661/1024\n",
            "404/404 [==============================] - 0s 109us/sample - loss: 37.3072 - mae: 4.5323 - val_loss: 45.4714 - val_mae: 4.9461\n",
            "Epoch 662/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 37.4375 - mae: 4.7544 - val_loss: 41.6220 - val_mae: 4.6456\n",
            "Epoch 663/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 39.2831 - mae: 4.5731 - val_loss: 39.1546 - val_mae: 4.4715\n",
            "Epoch 664/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 37.9164 - mae: 4.5496 - val_loss: 39.7423 - val_mae: 4.4985\n",
            "Epoch 665/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 37.6694 - mae: 4.7463 - val_loss: 42.5178 - val_mae: 4.7056\n",
            "Epoch 666/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 34.1930 - mae: 4.3966 - val_loss: 43.5430 - val_mae: 4.7845\n",
            "Epoch 667/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 44.7555 - mae: 5.0131 - val_loss: 41.6460 - val_mae: 4.6304\n",
            "Epoch 668/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 38.6319 - mae: 4.6342 - val_loss: 38.9464 - val_mae: 4.4463\n",
            "Epoch 669/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 37.9561 - mae: 4.6903 - val_loss: 37.4157 - val_mae: 4.3402\n",
            "Epoch 670/1024\n",
            "404/404 [==============================] - 0s 107us/sample - loss: 35.0561 - mae: 4.4524 - val_loss: 38.5752 - val_mae: 4.4390\n",
            "Epoch 671/1024\n",
            "404/404 [==============================] - 0s 90us/sample - loss: 40.1652 - mae: 4.8164 - val_loss: 41.6191 - val_mae: 4.6709\n",
            "Epoch 672/1024\n",
            "404/404 [==============================] - 0s 113us/sample - loss: 34.9068 - mae: 4.3235 - val_loss: 44.6898 - val_mae: 4.9136\n",
            "Epoch 673/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 38.1661 - mae: 4.6369 - val_loss: 43.6802 - val_mae: 4.8466\n",
            "Epoch 674/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 36.1770 - mae: 4.5537 - val_loss: 38.9178 - val_mae: 4.5145\n",
            "Epoch 675/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 38.9974 - mae: 4.7299 - val_loss: 37.4680 - val_mae: 4.4023\n",
            "Epoch 676/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 35.8039 - mae: 4.6276 - val_loss: 38.0314 - val_mae: 4.4383\n",
            "Epoch 677/1024\n",
            "404/404 [==============================] - 0s 120us/sample - loss: 37.9966 - mae: 4.6047 - val_loss: 40.7559 - val_mae: 4.6276\n",
            "Epoch 678/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 35.4624 - mae: 4.4603 - val_loss: 43.4279 - val_mae: 4.8402\n",
            "Epoch 679/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 33.5256 - mae: 4.3943 - val_loss: 43.0396 - val_mae: 4.8156\n",
            "Epoch 680/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 35.6860 - mae: 4.4639 - val_loss: 41.7525 - val_mae: 4.7161\n",
            "Epoch 681/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 37.5310 - mae: 4.8518 - val_loss: 41.8639 - val_mae: 4.7270\n",
            "Epoch 682/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 37.2800 - mae: 4.7443 - val_loss: 41.3897 - val_mae: 4.6913\n",
            "Epoch 683/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 34.4567 - mae: 4.3261 - val_loss: 40.3863 - val_mae: 4.6099\n",
            "Epoch 684/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 38.6974 - mae: 4.5318 - val_loss: 41.4898 - val_mae: 4.6956\n",
            "Epoch 685/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 39.6293 - mae: 4.8419 - val_loss: 40.6193 - val_mae: 4.6267\n",
            "Epoch 686/1024\n",
            "404/404 [==============================] - 0s 115us/sample - loss: 32.2659 - mae: 4.2204 - val_loss: 40.6795 - val_mae: 4.6380\n",
            "Epoch 687/1024\n",
            "404/404 [==============================] - 0s 109us/sample - loss: 37.5964 - mae: 4.5881 - val_loss: 40.0588 - val_mae: 4.6076\n",
            "Epoch 688/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 37.9643 - mae: 4.6360 - val_loss: 37.1509 - val_mae: 4.3992\n",
            "Epoch 689/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 36.3218 - mae: 4.5658 - val_loss: 38.6497 - val_mae: 4.5130\n",
            "Epoch 690/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 36.5124 - mae: 4.4718 - val_loss: 41.4150 - val_mae: 4.7162\n",
            "Epoch 691/1024\n",
            "404/404 [==============================] - 0s 109us/sample - loss: 34.7789 - mae: 4.4374 - val_loss: 41.8842 - val_mae: 4.7498\n",
            "Epoch 692/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 39.1410 - mae: 4.6106 - val_loss: 42.9398 - val_mae: 4.8133\n",
            "Epoch 693/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 35.2599 - mae: 4.4943 - val_loss: 43.0470 - val_mae: 4.8115\n",
            "Epoch 694/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 34.3436 - mae: 4.4502 - val_loss: 41.9383 - val_mae: 4.7186\n",
            "Epoch 695/1024\n",
            "404/404 [==============================] - 0s 108us/sample - loss: 33.1477 - mae: 4.4130 - val_loss: 38.4986 - val_mae: 4.4551\n",
            "Epoch 696/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 34.5422 - mae: 4.4598 - val_loss: 37.6587 - val_mae: 4.3882\n",
            "Epoch 697/1024\n",
            "404/404 [==============================] - 0s 109us/sample - loss: 40.8652 - mae: 4.6086 - val_loss: 41.0939 - val_mae: 4.6288\n",
            "Epoch 698/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 38.3130 - mae: 4.6848 - val_loss: 44.1636 - val_mae: 4.8622\n",
            "Epoch 699/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 31.4265 - mae: 4.3060 - val_loss: 44.7859 - val_mae: 4.9134\n",
            "Epoch 700/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 32.8117 - mae: 4.3439 - val_loss: 41.2066 - val_mae: 4.6426\n",
            "Epoch 701/1024\n",
            "404/404 [==============================] - 0s 107us/sample - loss: 32.0842 - mae: 4.2920 - val_loss: 37.5249 - val_mae: 4.3608\n",
            "Epoch 702/1024\n",
            "404/404 [==============================] - 0s 126us/sample - loss: 35.3665 - mae: 4.6006 - val_loss: 35.7832 - val_mae: 4.2489\n",
            "Epoch 703/1024\n",
            "404/404 [==============================] - 0s 137us/sample - loss: 34.8524 - mae: 4.6070 - val_loss: 36.0822 - val_mae: 4.2924\n",
            "Epoch 704/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 38.9358 - mae: 4.6280 - val_loss: 41.9511 - val_mae: 4.7526\n",
            "Epoch 705/1024\n",
            "404/404 [==============================] - 0s 105us/sample - loss: 37.6106 - mae: 4.6267 - val_loss: 50.1283 - val_mae: 5.3761\n",
            "Epoch 706/1024\n",
            "404/404 [==============================] - 0s 112us/sample - loss: 36.4467 - mae: 4.5888 - val_loss: 50.4345 - val_mae: 5.4021\n",
            "Epoch 707/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 37.9007 - mae: 4.5821 - val_loss: 48.5938 - val_mae: 5.2738\n",
            "Epoch 708/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 35.5600 - mae: 4.4151 - val_loss: 40.9189 - val_mae: 4.6861\n",
            "Epoch 709/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 39.5218 - mae: 4.8372 - val_loss: 36.0470 - val_mae: 4.3215\n",
            "Epoch 710/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 36.0084 - mae: 4.5460 - val_loss: 35.2894 - val_mae: 4.2564\n",
            "Epoch 711/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 36.8662 - mae: 4.6641 - val_loss: 37.9801 - val_mae: 4.4585\n",
            "Epoch 712/1024\n",
            "404/404 [==============================] - 0s 117us/sample - loss: 37.1445 - mae: 4.5560 - val_loss: 44.1429 - val_mae: 4.9274\n",
            "Epoch 713/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 35.0415 - mae: 4.4443 - val_loss: 48.2820 - val_mae: 5.2434\n",
            "Epoch 714/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 34.2563 - mae: 4.4220 - val_loss: 48.0251 - val_mae: 5.2176\n",
            "Epoch 715/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 38.0263 - mae: 4.6115 - val_loss: 42.8150 - val_mae: 4.7953\n",
            "Epoch 716/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 40.2626 - mae: 4.7196 - val_loss: 37.1966 - val_mae: 4.3370\n",
            "Epoch 717/1024\n",
            "404/404 [==============================] - 0s 88us/sample - loss: 32.1079 - mae: 4.3677 - val_loss: 34.6050 - val_mae: 4.1322\n",
            "Epoch 718/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 38.9889 - mae: 4.6183 - val_loss: 36.0799 - val_mae: 4.2160\n",
            "Epoch 719/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 30.5222 - mae: 4.2058 - val_loss: 38.4806 - val_mae: 4.4035\n",
            "Epoch 720/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 33.9715 - mae: 4.4345 - val_loss: 42.4324 - val_mae: 4.7266\n",
            "Epoch 721/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 33.3556 - mae: 4.4234 - val_loss: 44.5569 - val_mae: 4.8940\n",
            "Epoch 722/1024\n",
            "404/404 [==============================] - 0s 114us/sample - loss: 37.1105 - mae: 4.5906 - val_loss: 42.7692 - val_mae: 4.7648\n",
            "Epoch 723/1024\n",
            "404/404 [==============================] - 0s 105us/sample - loss: 37.8651 - mae: 4.6243 - val_loss: 41.0012 - val_mae: 4.6305\n",
            "Epoch 724/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 34.4651 - mae: 4.4439 - val_loss: 38.0297 - val_mae: 4.3958\n",
            "Epoch 725/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 35.1586 - mae: 4.4526 - val_loss: 37.6856 - val_mae: 4.3812\n",
            "Epoch 726/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 35.9094 - mae: 4.5504 - val_loss: 39.4874 - val_mae: 4.5405\n",
            "Epoch 727/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 34.5179 - mae: 4.5099 - val_loss: 42.2730 - val_mae: 4.7600\n",
            "Epoch 728/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 33.9766 - mae: 4.3104 - val_loss: 43.8801 - val_mae: 4.8934\n",
            "Epoch 729/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 35.3413 - mae: 4.4230 - val_loss: 41.3194 - val_mae: 4.6955\n",
            "Epoch 730/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 34.3357 - mae: 4.3725 - val_loss: 39.4820 - val_mae: 4.5505\n",
            "Epoch 731/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 37.1936 - mae: 4.5098 - val_loss: 35.9022 - val_mae: 4.2756\n",
            "Epoch 732/1024\n",
            "404/404 [==============================] - 0s 108us/sample - loss: 30.1816 - mae: 4.2845 - val_loss: 36.1294 - val_mae: 4.2840\n",
            "Epoch 733/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 39.8888 - mae: 4.8386 - val_loss: 37.9462 - val_mae: 4.4172\n",
            "Epoch 734/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 36.5688 - mae: 4.6152 - val_loss: 42.3970 - val_mae: 4.7564\n",
            "Epoch 735/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 32.9391 - mae: 4.4631 - val_loss: 44.9429 - val_mae: 4.9637\n",
            "Epoch 736/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 29.3845 - mae: 4.2486 - val_loss: 43.5357 - val_mae: 4.8488\n",
            "Epoch 737/1024\n",
            "404/404 [==============================] - 0s 85us/sample - loss: 36.4193 - mae: 4.4521 - val_loss: 40.8398 - val_mae: 4.6299\n",
            "Epoch 738/1024\n",
            "404/404 [==============================] - 0s 124us/sample - loss: 40.2136 - mae: 4.5503 - val_loss: 38.6403 - val_mae: 4.4448\n",
            "Epoch 739/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 34.8344 - mae: 4.4740 - val_loss: 38.4538 - val_mae: 4.4322\n",
            "Epoch 740/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 35.8018 - mae: 4.6682 - val_loss: 38.4437 - val_mae: 4.4476\n",
            "Epoch 741/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 38.3848 - mae: 4.6343 - val_loss: 41.6132 - val_mae: 4.6895\n",
            "Epoch 742/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 35.5650 - mae: 4.5046 - val_loss: 43.9536 - val_mae: 4.8741\n",
            "Epoch 743/1024\n",
            "404/404 [==============================] - 0s 107us/sample - loss: 34.6046 - mae: 4.2561 - val_loss: 41.0656 - val_mae: 4.6555\n",
            "Epoch 744/1024\n",
            "404/404 [==============================] - 0s 86us/sample - loss: 37.2324 - mae: 4.5656 - val_loss: 39.9088 - val_mae: 4.5801\n",
            "Epoch 745/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 32.3154 - mae: 4.3642 - val_loss: 37.9971 - val_mae: 4.4413\n",
            "Epoch 746/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 36.4234 - mae: 4.4985 - val_loss: 36.0913 - val_mae: 4.2903\n",
            "Epoch 747/1024\n",
            "404/404 [==============================] - 0s 89us/sample - loss: 37.3796 - mae: 4.5370 - val_loss: 37.6733 - val_mae: 4.4140\n",
            "Epoch 748/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 36.1818 - mae: 4.4056 - val_loss: 39.9649 - val_mae: 4.5783\n",
            "Epoch 749/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 36.4733 - mae: 4.4225 - val_loss: 42.9007 - val_mae: 4.7984\n",
            "Epoch 750/1024\n",
            "404/404 [==============================] - 0s 105us/sample - loss: 36.0361 - mae: 4.4537 - val_loss: 42.0901 - val_mae: 4.7326\n",
            "Epoch 751/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 32.4444 - mae: 4.2571 - val_loss: 40.8074 - val_mae: 4.6240\n",
            "Epoch 752/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 32.8616 - mae: 4.4135 - val_loss: 38.3867 - val_mae: 4.4376\n",
            "Epoch 753/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 33.7863 - mae: 4.4173 - val_loss: 37.4097 - val_mae: 4.3622\n",
            "Epoch 754/1024\n",
            "404/404 [==============================] - 0s 113us/sample - loss: 31.3451 - mae: 4.3461 - val_loss: 37.4658 - val_mae: 4.3623\n",
            "Epoch 755/1024\n",
            "404/404 [==============================] - 0s 108us/sample - loss: 33.7271 - mae: 4.4467 - val_loss: 39.6768 - val_mae: 4.5265\n",
            "Epoch 756/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 38.2638 - mae: 4.6584 - val_loss: 40.6781 - val_mae: 4.6077\n",
            "Epoch 757/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 36.8390 - mae: 4.5258 - val_loss: 41.9399 - val_mae: 4.7115\n",
            "Epoch 758/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 37.7643 - mae: 4.5873 - val_loss: 41.5481 - val_mae: 4.6868\n",
            "Epoch 759/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 31.1861 - mae: 4.2188 - val_loss: 38.5218 - val_mae: 4.4574\n",
            "Epoch 760/1024\n",
            "404/404 [==============================] - 0s 109us/sample - loss: 32.8588 - mae: 4.4160 - val_loss: 34.5689 - val_mae: 4.1612\n",
            "Epoch 761/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 35.7288 - mae: 4.4361 - val_loss: 33.5459 - val_mae: 4.0802\n",
            "Epoch 762/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 36.7528 - mae: 4.5973 - val_loss: 34.7127 - val_mae: 4.1678\n",
            "Epoch 763/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 38.0233 - mae: 4.6096 - val_loss: 39.3394 - val_mae: 4.5285\n",
            "Epoch 764/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 35.7841 - mae: 4.3429 - val_loss: 44.4778 - val_mae: 4.9445\n",
            "Epoch 765/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 35.4511 - mae: 4.4100 - val_loss: 47.1010 - val_mae: 5.1477\n",
            "Epoch 766/1024\n",
            "404/404 [==============================] - 0s 109us/sample - loss: 37.0575 - mae: 4.6212 - val_loss: 42.6297 - val_mae: 4.7854\n",
            "Epoch 767/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 36.9681 - mae: 4.4514 - val_loss: 37.0426 - val_mae: 4.3392\n",
            "Epoch 768/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 35.1539 - mae: 4.4813 - val_loss: 34.2361 - val_mae: 4.1092\n",
            "Epoch 769/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 33.1183 - mae: 4.3608 - val_loss: 33.8194 - val_mae: 4.0764\n",
            "Epoch 770/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 34.0676 - mae: 4.4854 - val_loss: 35.2806 - val_mae: 4.2028\n",
            "Epoch 771/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 29.5613 - mae: 4.2057 - val_loss: 38.4347 - val_mae: 4.4607\n",
            "Epoch 772/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 35.7557 - mae: 4.5316 - val_loss: 40.2576 - val_mae: 4.6050\n",
            "Epoch 773/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 31.5734 - mae: 4.2616 - val_loss: 39.6864 - val_mae: 4.5583\n",
            "Epoch 774/1024\n",
            "404/404 [==============================] - 0s 114us/sample - loss: 32.3001 - mae: 4.2020 - val_loss: 38.1559 - val_mae: 4.4302\n",
            "Epoch 775/1024\n",
            "404/404 [==============================] - 0s 106us/sample - loss: 36.6931 - mae: 4.4554 - val_loss: 36.6203 - val_mae: 4.3015\n",
            "Epoch 776/1024\n",
            "404/404 [==============================] - 0s 105us/sample - loss: 31.3601 - mae: 4.3217 - val_loss: 35.2997 - val_mae: 4.2004\n",
            "Epoch 777/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 35.1472 - mae: 4.3471 - val_loss: 35.3167 - val_mae: 4.2070\n",
            "Epoch 778/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 32.7958 - mae: 4.3004 - val_loss: 37.3859 - val_mae: 4.3807\n",
            "Epoch 779/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 34.8191 - mae: 4.3907 - val_loss: 37.6689 - val_mae: 4.4080\n",
            "Epoch 780/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 37.7002 - mae: 4.5584 - val_loss: 38.2835 - val_mae: 4.4621\n",
            "Epoch 781/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 31.3454 - mae: 4.2358 - val_loss: 37.9172 - val_mae: 4.4323\n",
            "Epoch 782/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 35.2648 - mae: 4.4466 - val_loss: 37.0103 - val_mae: 4.3572\n",
            "Epoch 783/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 36.8546 - mae: 4.5876 - val_loss: 36.9131 - val_mae: 4.3505\n",
            "Epoch 784/1024\n",
            "404/404 [==============================] - 0s 89us/sample - loss: 37.1960 - mae: 4.3951 - val_loss: 36.6234 - val_mae: 4.3291\n",
            "Epoch 785/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 36.3480 - mae: 4.5589 - val_loss: 35.1596 - val_mae: 4.2167\n",
            "Epoch 786/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 39.9454 - mae: 4.7896 - val_loss: 37.5379 - val_mae: 4.3997\n",
            "Epoch 787/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 35.6518 - mae: 4.5079 - val_loss: 41.0513 - val_mae: 4.6885\n",
            "Epoch 788/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 31.6699 - mae: 4.4103 - val_loss: 43.0312 - val_mae: 4.8390\n",
            "Epoch 789/1024\n",
            "404/404 [==============================] - 0s 109us/sample - loss: 33.7082 - mae: 4.5071 - val_loss: 43.8057 - val_mae: 4.8991\n",
            "Epoch 790/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 33.5564 - mae: 4.3521 - val_loss: 42.4995 - val_mae: 4.7932\n",
            "Epoch 791/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 30.1703 - mae: 4.1534 - val_loss: 40.0663 - val_mae: 4.5909\n",
            "Epoch 792/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 33.2017 - mae: 4.3703 - val_loss: 38.7943 - val_mae: 4.4801\n",
            "Epoch 793/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 38.7702 - mae: 4.6632 - val_loss: 39.2475 - val_mae: 4.5130\n",
            "Epoch 794/1024\n",
            "404/404 [==============================] - 0s 105us/sample - loss: 32.8190 - mae: 4.4142 - val_loss: 39.9130 - val_mae: 4.5554\n",
            "Epoch 795/1024\n",
            "404/404 [==============================] - 0s 120us/sample - loss: 31.8960 - mae: 4.3559 - val_loss: 39.7851 - val_mae: 4.5426\n",
            "Epoch 796/1024\n",
            "404/404 [==============================] - 0s 107us/sample - loss: 41.1340 - mae: 4.6737 - val_loss: 36.9531 - val_mae: 4.3163\n",
            "Epoch 797/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 37.1554 - mae: 4.5339 - val_loss: 37.1072 - val_mae: 4.3268\n",
            "Epoch 798/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 31.7252 - mae: 4.1205 - val_loss: 37.3880 - val_mae: 4.3397\n",
            "Epoch 799/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 37.3490 - mae: 4.4532 - val_loss: 38.1206 - val_mae: 4.4056\n",
            "Epoch 800/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 31.2551 - mae: 4.3378 - val_loss: 39.8722 - val_mae: 4.5525\n",
            "Epoch 801/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 33.8106 - mae: 4.4420 - val_loss: 39.1225 - val_mae: 4.5070\n",
            "Epoch 802/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 36.2127 - mae: 4.4523 - val_loss: 37.5503 - val_mae: 4.3871\n",
            "Epoch 803/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 37.2160 - mae: 4.4942 - val_loss: 38.2310 - val_mae: 4.4464\n",
            "Epoch 804/1024\n",
            "404/404 [==============================] - 0s 108us/sample - loss: 37.5830 - mae: 4.6450 - val_loss: 38.4709 - val_mae: 4.4830\n",
            "Epoch 805/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 35.8739 - mae: 4.5462 - val_loss: 39.6299 - val_mae: 4.5845\n",
            "Epoch 806/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 38.3776 - mae: 4.6919 - val_loss: 42.6763 - val_mae: 4.8214\n",
            "Epoch 807/1024\n",
            "404/404 [==============================] - 0s 89us/sample - loss: 36.4278 - mae: 4.5092 - val_loss: 41.8403 - val_mae: 4.7613\n",
            "Epoch 808/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 34.3275 - mae: 4.2626 - val_loss: 41.4457 - val_mae: 4.7270\n",
            "Epoch 809/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 33.9601 - mae: 4.3412 - val_loss: 38.8693 - val_mae: 4.5334\n",
            "Epoch 810/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 33.9306 - mae: 4.3965 - val_loss: 35.3709 - val_mae: 4.2327\n",
            "Epoch 811/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 34.3794 - mae: 4.5378 - val_loss: 34.8151 - val_mae: 4.1758\n",
            "Epoch 812/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 32.6901 - mae: 4.4775 - val_loss: 37.3804 - val_mae: 4.4053\n",
            "Epoch 813/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 34.5792 - mae: 4.3722 - val_loss: 43.2966 - val_mae: 4.8644\n",
            "Epoch 814/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 37.3738 - mae: 4.5687 - val_loss: 46.0714 - val_mae: 5.0839\n",
            "Epoch 815/1024\n",
            "404/404 [==============================] - 0s 85us/sample - loss: 33.8368 - mae: 4.4363 - val_loss: 43.7348 - val_mae: 4.9110\n",
            "Epoch 816/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 31.0411 - mae: 4.1435 - val_loss: 38.6581 - val_mae: 4.5120\n",
            "Epoch 817/1024\n",
            "404/404 [==============================] - 0s 123us/sample - loss: 32.1953 - mae: 4.3443 - val_loss: 34.0231 - val_mae: 4.1021\n",
            "Epoch 818/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 38.7599 - mae: 4.6028 - val_loss: 34.3206 - val_mae: 4.1349\n",
            "Epoch 819/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 41.1821 - mae: 4.5617 - val_loss: 39.1211 - val_mae: 4.5449\n",
            "Epoch 820/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 34.2076 - mae: 4.2950 - val_loss: 45.9834 - val_mae: 5.0816\n",
            "Epoch 821/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 33.7882 - mae: 4.3610 - val_loss: 47.8759 - val_mae: 5.2094\n",
            "Epoch 822/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 31.1910 - mae: 4.1688 - val_loss: 43.3084 - val_mae: 4.8452\n",
            "Epoch 823/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 30.8855 - mae: 4.1707 - val_loss: 36.6055 - val_mae: 4.2944\n",
            "Epoch 824/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 35.8193 - mae: 4.5163 - val_loss: 33.7896 - val_mae: 4.0562\n",
            "Epoch 825/1024\n",
            "404/404 [==============================] - 0s 89us/sample - loss: 37.9262 - mae: 4.6745 - val_loss: 35.0669 - val_mae: 4.1551\n",
            "Epoch 826/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 33.2804 - mae: 4.3355 - val_loss: 37.5694 - val_mae: 4.3711\n",
            "Epoch 827/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 32.6849 - mae: 4.2816 - val_loss: 38.7624 - val_mae: 4.4729\n",
            "Epoch 828/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 35.8818 - mae: 4.4830 - val_loss: 41.1762 - val_mae: 4.6765\n",
            "Epoch 829/1024\n",
            "404/404 [==============================] - 0s 105us/sample - loss: 30.9652 - mae: 4.1616 - val_loss: 38.8068 - val_mae: 4.4818\n",
            "Epoch 830/1024\n",
            "404/404 [==============================] - 0s 87us/sample - loss: 33.6927 - mae: 4.3139 - val_loss: 34.9793 - val_mae: 4.1787\n",
            "Epoch 831/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 39.5935 - mae: 4.6512 - val_loss: 33.7507 - val_mae: 4.0755\n",
            "Epoch 832/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 31.3320 - mae: 4.2423 - val_loss: 33.6242 - val_mae: 4.0736\n",
            "Epoch 833/1024\n",
            "404/404 [==============================] - 0s 89us/sample - loss: 36.4606 - mae: 4.5655 - val_loss: 36.1847 - val_mae: 4.2903\n",
            "Epoch 834/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 32.3807 - mae: 4.3514 - val_loss: 42.4413 - val_mae: 4.8227\n",
            "Epoch 835/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 35.0654 - mae: 4.5289 - val_loss: 43.4780 - val_mae: 4.9049\n",
            "Epoch 836/1024\n",
            "404/404 [==============================] - 0s 118us/sample - loss: 34.2430 - mae: 4.1501 - val_loss: 40.0109 - val_mae: 4.6186\n",
            "Epoch 837/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 31.6864 - mae: 4.1224 - val_loss: 36.2841 - val_mae: 4.3011\n",
            "Epoch 838/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 32.1738 - mae: 4.3148 - val_loss: 34.8322 - val_mae: 4.1755\n",
            "Epoch 839/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 31.0252 - mae: 4.1955 - val_loss: 34.7741 - val_mae: 4.1753\n",
            "Epoch 840/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 38.1387 - mae: 4.6312 - val_loss: 37.0654 - val_mae: 4.3770\n",
            "Epoch 841/1024\n",
            "404/404 [==============================] - 0s 89us/sample - loss: 30.5203 - mae: 4.1676 - val_loss: 39.9062 - val_mae: 4.6122\n",
            "Epoch 842/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 31.5722 - mae: 4.2473 - val_loss: 40.4444 - val_mae: 4.6680\n",
            "Epoch 843/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 31.1230 - mae: 4.1904 - val_loss: 39.7572 - val_mae: 4.6217\n",
            "Epoch 844/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 33.8752 - mae: 4.3738 - val_loss: 37.5864 - val_mae: 4.4474\n",
            "Epoch 845/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 32.1996 - mae: 4.3716 - val_loss: 36.9422 - val_mae: 4.3905\n",
            "Epoch 846/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 32.9646 - mae: 4.2727 - val_loss: 36.1642 - val_mae: 4.3200\n",
            "Epoch 847/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 30.9979 - mae: 4.2110 - val_loss: 37.0475 - val_mae: 4.3889\n",
            "Epoch 848/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 32.0991 - mae: 4.3526 - val_loss: 38.4833 - val_mae: 4.4876\n",
            "Epoch 849/1024\n",
            "404/404 [==============================] - 0s 88us/sample - loss: 30.6508 - mae: 4.1846 - val_loss: 40.4525 - val_mae: 4.6376\n",
            "Epoch 850/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 33.6217 - mae: 4.3905 - val_loss: 39.3546 - val_mae: 4.5367\n",
            "Epoch 851/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 35.1581 - mae: 4.4549 - val_loss: 36.8745 - val_mae: 4.3320\n",
            "Epoch 852/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 34.7332 - mae: 4.4530 - val_loss: 34.2842 - val_mae: 4.1155\n",
            "Epoch 853/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 36.1061 - mae: 4.4481 - val_loss: 33.3190 - val_mae: 4.0370\n",
            "Epoch 854/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 30.2317 - mae: 4.2153 - val_loss: 33.8805 - val_mae: 4.0921\n",
            "Epoch 855/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 37.7933 - mae: 4.5693 - val_loss: 35.6079 - val_mae: 4.2395\n",
            "Epoch 856/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 31.8635 - mae: 4.3538 - val_loss: 38.1877 - val_mae: 4.4622\n",
            "Epoch 857/1024\n",
            "404/404 [==============================] - 0s 89us/sample - loss: 28.7421 - mae: 4.1228 - val_loss: 39.8051 - val_mae: 4.6064\n",
            "Epoch 858/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 32.7360 - mae: 4.2788 - val_loss: 39.9611 - val_mae: 4.6304\n",
            "Epoch 859/1024\n",
            "404/404 [==============================] - 0s 114us/sample - loss: 35.1443 - mae: 4.5256 - val_loss: 38.1903 - val_mae: 4.4983\n",
            "Epoch 860/1024\n",
            "404/404 [==============================] - 0s 90us/sample - loss: 32.3102 - mae: 4.3132 - val_loss: 36.3159 - val_mae: 4.3417\n",
            "Epoch 861/1024\n",
            "404/404 [==============================] - 0s 113us/sample - loss: 29.6958 - mae: 4.1051 - val_loss: 35.5948 - val_mae: 4.2791\n",
            "Epoch 862/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 28.1411 - mae: 4.0918 - val_loss: 36.3393 - val_mae: 4.3422\n",
            "Epoch 863/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 36.8557 - mae: 4.5491 - val_loss: 39.2553 - val_mae: 4.5839\n",
            "Epoch 864/1024\n",
            "404/404 [==============================] - 0s 84us/sample - loss: 31.2885 - mae: 4.3866 - val_loss: 41.0607 - val_mae: 4.7210\n",
            "Epoch 865/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 35.1508 - mae: 4.4062 - val_loss: 39.2548 - val_mae: 4.5511\n",
            "Epoch 866/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 33.1021 - mae: 4.4250 - val_loss: 36.3889 - val_mae: 4.3003\n",
            "Epoch 867/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 36.5691 - mae: 4.3994 - val_loss: 35.1481 - val_mae: 4.1812\n",
            "Epoch 868/1024\n",
            "404/404 [==============================] - 0s 90us/sample - loss: 29.9749 - mae: 4.1793 - val_loss: 35.6194 - val_mae: 4.2001\n",
            "Epoch 869/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 35.8042 - mae: 4.5603 - val_loss: 35.6073 - val_mae: 4.1865\n",
            "Epoch 870/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 28.7669 - mae: 4.1562 - val_loss: 36.3142 - val_mae: 4.2452\n",
            "Epoch 871/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 29.4404 - mae: 4.0832 - val_loss: 37.3459 - val_mae: 4.3401\n",
            "Epoch 872/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 30.6698 - mae: 4.2049 - val_loss: 38.1376 - val_mae: 4.4281\n",
            "Epoch 873/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 32.8306 - mae: 4.2954 - val_loss: 38.5307 - val_mae: 4.4764\n",
            "Epoch 874/1024\n",
            "404/404 [==============================] - 0s 89us/sample - loss: 33.7700 - mae: 4.2614 - val_loss: 37.1185 - val_mae: 4.3790\n",
            "Epoch 875/1024\n",
            "404/404 [==============================] - 0s 109us/sample - loss: 31.3589 - mae: 4.2446 - val_loss: 35.9436 - val_mae: 4.2901\n",
            "Epoch 876/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 32.1440 - mae: 4.3026 - val_loss: 35.7540 - val_mae: 4.2817\n",
            "Epoch 877/1024\n",
            "404/404 [==============================] - 0s 88us/sample - loss: 33.2869 - mae: 4.3455 - val_loss: 35.7810 - val_mae: 4.2858\n",
            "Epoch 878/1024\n",
            "404/404 [==============================] - 0s 82us/sample - loss: 27.8848 - mae: 4.0879 - val_loss: 35.6791 - val_mae: 4.2810\n",
            "Epoch 879/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 34.8468 - mae: 4.4291 - val_loss: 36.3997 - val_mae: 4.3438\n",
            "Epoch 880/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 32.7930 - mae: 4.4256 - val_loss: 37.5891 - val_mae: 4.4439\n",
            "Epoch 881/1024\n",
            "404/404 [==============================] - 0s 92us/sample - loss: 32.0222 - mae: 4.2760 - val_loss: 37.8650 - val_mae: 4.4714\n",
            "Epoch 882/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 28.5912 - mae: 4.1958 - val_loss: 37.3945 - val_mae: 4.4329\n",
            "Epoch 883/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 30.2685 - mae: 4.2090 - val_loss: 38.0843 - val_mae: 4.4877\n",
            "Epoch 884/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 32.5697 - mae: 4.2373 - val_loss: 37.0047 - val_mae: 4.3901\n",
            "Epoch 885/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 31.1848 - mae: 4.1839 - val_loss: 35.7111 - val_mae: 4.2702\n",
            "Epoch 886/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 30.5720 - mae: 4.1033 - val_loss: 34.3027 - val_mae: 4.1421\n",
            "Epoch 887/1024\n",
            "404/404 [==============================] - 0s 80us/sample - loss: 33.9146 - mae: 4.3396 - val_loss: 33.2595 - val_mae: 4.0437\n",
            "Epoch 888/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 30.9291 - mae: 4.4140 - val_loss: 34.1267 - val_mae: 4.1135\n",
            "Epoch 889/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 26.1965 - mae: 3.9216 - val_loss: 35.7903 - val_mae: 4.2590\n",
            "Epoch 890/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 30.6418 - mae: 4.1018 - val_loss: 37.5574 - val_mae: 4.4127\n",
            "Epoch 891/1024\n",
            "404/404 [==============================] - 0s 108us/sample - loss: 31.6889 - mae: 4.1623 - val_loss: 37.5159 - val_mae: 4.4094\n",
            "Epoch 892/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 32.9684 - mae: 4.2644 - val_loss: 36.8457 - val_mae: 4.3505\n",
            "Epoch 893/1024\n",
            "404/404 [==============================] - 0s 88us/sample - loss: 30.0942 - mae: 4.2044 - val_loss: 37.1920 - val_mae: 4.3763\n",
            "Epoch 894/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 26.3224 - mae: 3.8903 - val_loss: 36.6587 - val_mae: 4.3320\n",
            "Epoch 895/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 30.2732 - mae: 4.1276 - val_loss: 36.4160 - val_mae: 4.3156\n",
            "Epoch 896/1024\n",
            "404/404 [==============================] - 0s 104us/sample - loss: 32.7946 - mae: 4.2348 - val_loss: 36.1304 - val_mae: 4.2909\n",
            "Epoch 897/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 30.7681 - mae: 4.2232 - val_loss: 36.4841 - val_mae: 4.3267\n",
            "Epoch 898/1024\n",
            "404/404 [==============================] - 0s 85us/sample - loss: 29.4016 - mae: 4.0697 - val_loss: 37.6874 - val_mae: 4.4367\n",
            "Epoch 899/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 33.0330 - mae: 4.3745 - val_loss: 38.3133 - val_mae: 4.4941\n",
            "Epoch 900/1024\n",
            "404/404 [==============================] - 0s 88us/sample - loss: 34.3464 - mae: 4.4494 - val_loss: 37.8119 - val_mae: 4.4480\n",
            "Epoch 901/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 34.0283 - mae: 4.2890 - val_loss: 37.9277 - val_mae: 4.4558\n",
            "Epoch 902/1024\n",
            "404/404 [==============================] - 0s 90us/sample - loss: 32.9741 - mae: 4.3675 - val_loss: 35.1445 - val_mae: 4.2050\n",
            "Epoch 903/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 33.0067 - mae: 4.2696 - val_loss: 32.6876 - val_mae: 3.9800\n",
            "Epoch 904/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 30.1185 - mae: 4.1288 - val_loss: 32.1784 - val_mae: 3.9305\n",
            "Epoch 905/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 33.5175 - mae: 4.4038 - val_loss: 34.0306 - val_mae: 4.0939\n",
            "Epoch 906/1024\n",
            "404/404 [==============================] - 0s 87us/sample - loss: 30.6136 - mae: 4.1243 - val_loss: 35.8548 - val_mae: 4.2400\n",
            "Epoch 907/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 28.4943 - mae: 4.0921 - val_loss: 38.0565 - val_mae: 4.4266\n",
            "Epoch 908/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 30.7061 - mae: 4.1623 - val_loss: 37.8428 - val_mae: 4.4079\n",
            "Epoch 909/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 28.5646 - mae: 4.0538 - val_loss: 35.5022 - val_mae: 4.2131\n",
            "Epoch 910/1024\n",
            "404/404 [==============================] - 0s 125us/sample - loss: 33.0072 - mae: 4.3997 - val_loss: 33.9245 - val_mae: 4.0899\n",
            "Epoch 911/1024\n",
            "404/404 [==============================] - 0s 107us/sample - loss: 32.5072 - mae: 4.2627 - val_loss: 33.2604 - val_mae: 4.0404\n",
            "Epoch 912/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 31.8702 - mae: 4.2712 - val_loss: 33.5432 - val_mae: 4.0650\n",
            "Epoch 913/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 32.9857 - mae: 4.2285 - val_loss: 36.1394 - val_mae: 4.3025\n",
            "Epoch 914/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 36.4363 - mae: 4.4756 - val_loss: 37.6693 - val_mae: 4.4417\n",
            "Epoch 915/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 31.1294 - mae: 4.3684 - val_loss: 38.9739 - val_mae: 4.5537\n",
            "Epoch 916/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 29.7423 - mae: 3.9744 - val_loss: 37.6946 - val_mae: 4.4501\n",
            "Epoch 917/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 34.5435 - mae: 4.3847 - val_loss: 35.7726 - val_mae: 4.2785\n",
            "Epoch 918/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 35.3464 - mae: 4.4166 - val_loss: 36.0013 - val_mae: 4.3009\n",
            "Epoch 919/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 33.9910 - mae: 4.3270 - val_loss: 35.8083 - val_mae: 4.2821\n",
            "Epoch 920/1024\n",
            "404/404 [==============================] - 0s 108us/sample - loss: 36.2830 - mae: 4.4563 - val_loss: 36.6490 - val_mae: 4.3600\n",
            "Epoch 921/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 31.8742 - mae: 4.2437 - val_loss: 38.3953 - val_mae: 4.5119\n",
            "Epoch 922/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 28.0508 - mae: 4.0268 - val_loss: 37.5397 - val_mae: 4.4343\n",
            "Epoch 923/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 31.3709 - mae: 4.2501 - val_loss: 37.0305 - val_mae: 4.3854\n",
            "Epoch 924/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 34.9632 - mae: 4.4690 - val_loss: 35.0511 - val_mae: 4.2017\n",
            "Epoch 925/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 29.2297 - mae: 4.1059 - val_loss: 34.4805 - val_mae: 4.1565\n",
            "Epoch 926/1024\n",
            "404/404 [==============================] - 0s 111us/sample - loss: 29.9839 - mae: 4.1501 - val_loss: 34.7892 - val_mae: 4.1887\n",
            "Epoch 927/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 32.6016 - mae: 4.3596 - val_loss: 36.6814 - val_mae: 4.3526\n",
            "Epoch 928/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 32.1006 - mae: 4.1380 - val_loss: 38.9383 - val_mae: 4.5329\n",
            "Epoch 929/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 28.8956 - mae: 4.0399 - val_loss: 40.1371 - val_mae: 4.6240\n",
            "Epoch 930/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 31.1667 - mae: 4.2361 - val_loss: 38.7521 - val_mae: 4.5175\n",
            "Epoch 931/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 32.6019 - mae: 4.3520 - val_loss: 36.3881 - val_mae: 4.3289\n",
            "Epoch 932/1024\n",
            "404/404 [==============================] - 0s 110us/sample - loss: 28.5466 - mae: 4.0357 - val_loss: 34.8174 - val_mae: 4.1875\n",
            "Epoch 933/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 36.7840 - mae: 4.4861 - val_loss: 35.6619 - val_mae: 4.2637\n",
            "Epoch 934/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 34.8681 - mae: 4.4720 - val_loss: 39.5713 - val_mae: 4.5790\n",
            "Epoch 935/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 31.2577 - mae: 4.1890 - val_loss: 41.3058 - val_mae: 4.7082\n",
            "Epoch 936/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 26.8608 - mae: 3.8805 - val_loss: 39.2614 - val_mae: 4.5404\n",
            "Epoch 937/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 34.7063 - mae: 4.3213 - val_loss: 35.4198 - val_mae: 4.2081\n",
            "Epoch 938/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 31.5703 - mae: 4.2011 - val_loss: 33.2493 - val_mae: 4.0123\n",
            "Epoch 939/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 35.0823 - mae: 4.3546 - val_loss: 32.5418 - val_mae: 3.9487\n",
            "Epoch 940/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 30.3515 - mae: 4.1651 - val_loss: 34.0123 - val_mae: 4.0870\n",
            "Epoch 941/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 30.1051 - mae: 4.2681 - val_loss: 36.9836 - val_mae: 4.3648\n",
            "Epoch 942/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 35.1313 - mae: 4.5167 - val_loss: 38.7154 - val_mae: 4.5176\n",
            "Epoch 943/1024\n",
            "404/404 [==============================] - 0s 106us/sample - loss: 32.8144 - mae: 4.1910 - val_loss: 37.8746 - val_mae: 4.4524\n",
            "Epoch 944/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 31.5469 - mae: 4.1982 - val_loss: 34.2599 - val_mae: 4.1251\n",
            "Epoch 945/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 33.5858 - mae: 4.3936 - val_loss: 32.6288 - val_mae: 3.9805\n",
            "Epoch 946/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 35.8656 - mae: 4.5684 - val_loss: 35.0229 - val_mae: 4.1888\n",
            "Epoch 947/1024\n",
            "404/404 [==============================] - 0s 150us/sample - loss: 33.0551 - mae: 4.3608 - val_loss: 40.2043 - val_mae: 4.6476\n",
            "Epoch 948/1024\n",
            "404/404 [==============================] - 0s 125us/sample - loss: 32.1253 - mae: 4.2826 - val_loss: 43.7928 - val_mae: 4.9435\n",
            "Epoch 949/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 32.2761 - mae: 4.1971 - val_loss: 42.3361 - val_mae: 4.8108\n",
            "Epoch 950/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 31.8463 - mae: 4.1811 - val_loss: 34.7166 - val_mae: 4.1473\n",
            "Epoch 951/1024\n",
            "404/404 [==============================] - 0s 103us/sample - loss: 35.0824 - mae: 4.3858 - val_loss: 31.3272 - val_mae: 3.8423\n",
            "Epoch 952/1024\n",
            "404/404 [==============================] - 0s 113us/sample - loss: 31.4461 - mae: 4.1000 - val_loss: 30.5859 - val_mae: 3.7783\n",
            "Epoch 953/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 34.5014 - mae: 4.5319 - val_loss: 31.9817 - val_mae: 3.9083\n",
            "Epoch 954/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 32.3860 - mae: 4.3055 - val_loss: 37.1917 - val_mae: 4.3939\n",
            "Epoch 955/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 30.6824 - mae: 4.1897 - val_loss: 42.2026 - val_mae: 4.8264\n",
            "Epoch 956/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 37.1982 - mae: 4.4752 - val_loss: 42.3109 - val_mae: 4.8478\n",
            "Epoch 957/1024\n",
            "404/404 [==============================] - 0s 90us/sample - loss: 35.8589 - mae: 4.3566 - val_loss: 38.1083 - val_mae: 4.5099\n",
            "Epoch 958/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 33.4157 - mae: 4.1427 - val_loss: 33.9725 - val_mae: 4.1287\n",
            "Epoch 959/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 34.4961 - mae: 4.4439 - val_loss: 32.3916 - val_mae: 3.9871\n",
            "Epoch 960/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 31.0315 - mae: 4.1661 - val_loss: 31.9467 - val_mae: 3.9520\n",
            "Epoch 961/1024\n",
            "404/404 [==============================] - 0s 117us/sample - loss: 39.2329 - mae: 4.6277 - val_loss: 33.5022 - val_mae: 4.0792\n",
            "Epoch 962/1024\n",
            "404/404 [==============================] - 0s 94us/sample - loss: 31.1157 - mae: 4.3418 - val_loss: 36.6098 - val_mae: 4.3614\n",
            "Epoch 963/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 26.9987 - mae: 4.0198 - val_loss: 40.1065 - val_mae: 4.6525\n",
            "Epoch 964/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 32.9542 - mae: 4.3181 - val_loss: 41.4177 - val_mae: 4.7557\n",
            "Epoch 965/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 33.1221 - mae: 4.3470 - val_loss: 38.5870 - val_mae: 4.5300\n",
            "Epoch 966/1024\n",
            "404/404 [==============================] - 0s 86us/sample - loss: 28.9508 - mae: 4.0923 - val_loss: 36.5377 - val_mae: 4.3590\n",
            "Epoch 967/1024\n",
            "404/404 [==============================] - 0s 106us/sample - loss: 33.3063 - mae: 4.2120 - val_loss: 34.6523 - val_mae: 4.2027\n",
            "Epoch 968/1024\n",
            "404/404 [==============================] - 0s 105us/sample - loss: 35.1177 - mae: 4.2522 - val_loss: 35.7676 - val_mae: 4.3003\n",
            "Epoch 969/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 31.1627 - mae: 4.2640 - val_loss: 37.9353 - val_mae: 4.4734\n",
            "Epoch 970/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 29.8715 - mae: 4.1001 - val_loss: 40.1537 - val_mae: 4.6457\n",
            "Epoch 971/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 29.4896 - mae: 4.0403 - val_loss: 38.7090 - val_mae: 4.5276\n",
            "Epoch 972/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 30.4325 - mae: 4.1033 - val_loss: 35.9683 - val_mae: 4.2957\n",
            "Epoch 973/1024\n",
            "404/404 [==============================] - 0s 87us/sample - loss: 30.5883 - mae: 4.3171 - val_loss: 33.6850 - val_mae: 4.0812\n",
            "Epoch 974/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 33.6000 - mae: 4.3670 - val_loss: 33.8922 - val_mae: 4.1016\n",
            "Epoch 975/1024\n",
            "404/404 [==============================] - 0s 90us/sample - loss: 28.9832 - mae: 4.0045 - val_loss: 35.2299 - val_mae: 4.2208\n",
            "Epoch 976/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 33.1877 - mae: 4.3091 - val_loss: 37.7648 - val_mae: 4.4462\n",
            "Epoch 977/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 30.8407 - mae: 4.1742 - val_loss: 39.1362 - val_mae: 4.5528\n",
            "Epoch 978/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 33.0934 - mae: 4.3036 - val_loss: 38.5416 - val_mae: 4.4955\n",
            "Epoch 979/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 33.1647 - mae: 4.0778 - val_loss: 35.5307 - val_mae: 4.2287\n",
            "Epoch 980/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 32.4352 - mae: 4.2240 - val_loss: 32.9330 - val_mae: 3.9926\n",
            "Epoch 981/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 29.1663 - mae: 4.0343 - val_loss: 30.7481 - val_mae: 3.8019\n",
            "Epoch 982/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 32.4567 - mae: 4.3960 - val_loss: 30.2030 - val_mae: 3.7666\n",
            "Epoch 983/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 29.6357 - mae: 4.1883 - val_loss: 30.2340 - val_mae: 3.7825\n",
            "Epoch 984/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 30.3949 - mae: 4.0043 - val_loss: 31.9592 - val_mae: 3.9446\n",
            "Epoch 985/1024\n",
            "404/404 [==============================] - 0s 108us/sample - loss: 34.0652 - mae: 4.3224 - val_loss: 37.1217 - val_mae: 4.4340\n",
            "Epoch 986/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 31.3659 - mae: 4.2289 - val_loss: 41.7281 - val_mae: 4.8421\n",
            "Epoch 987/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 31.8876 - mae: 4.2187 - val_loss: 43.4646 - val_mae: 4.9824\n",
            "Epoch 988/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 32.5131 - mae: 4.3168 - val_loss: 39.4774 - val_mae: 4.6556\n",
            "Epoch 989/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 27.7419 - mae: 4.0118 - val_loss: 33.8950 - val_mae: 4.1585\n",
            "Epoch 990/1024\n",
            "404/404 [==============================] - 0s 102us/sample - loss: 35.3146 - mae: 4.4860 - val_loss: 32.0402 - val_mae: 3.9687\n",
            "Epoch 991/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 31.2837 - mae: 4.2473 - val_loss: 31.6967 - val_mae: 3.9421\n",
            "Epoch 992/1024\n",
            "404/404 [==============================] - 0s 121us/sample - loss: 30.7173 - mae: 4.1653 - val_loss: 32.8534 - val_mae: 4.0499\n",
            "Epoch 993/1024\n",
            "404/404 [==============================] - 0s 107us/sample - loss: 31.1719 - mae: 4.1503 - val_loss: 35.3929 - val_mae: 4.2720\n",
            "Epoch 994/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 29.3054 - mae: 3.9830 - val_loss: 35.6685 - val_mae: 4.2849\n",
            "Epoch 995/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 26.0093 - mae: 3.9550 - val_loss: 35.3316 - val_mae: 4.2411\n",
            "Epoch 996/1024\n",
            "404/404 [==============================] - 0s 110us/sample - loss: 37.4768 - mae: 4.3783 - val_loss: 32.4700 - val_mae: 3.9756\n",
            "Epoch 997/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 26.5586 - mae: 3.8781 - val_loss: 31.1168 - val_mae: 3.8542\n",
            "Epoch 998/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 33.5560 - mae: 4.4176 - val_loss: 34.1446 - val_mae: 4.1033\n",
            "Epoch 999/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 31.8639 - mae: 4.1976 - val_loss: 37.6175 - val_mae: 4.3937\n",
            "Epoch 1000/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 30.6988 - mae: 4.0615 - val_loss: 38.0117 - val_mae: 4.4220\n",
            "Epoch 1001/1024\n",
            "404/404 [==============================] - 0s 88us/sample - loss: 29.9130 - mae: 4.1240 - val_loss: 36.2231 - val_mae: 4.2733\n",
            "Epoch 1002/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 29.1911 - mae: 4.0817 - val_loss: 33.9501 - val_mae: 4.0839\n",
            "Epoch 1003/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 28.6835 - mae: 4.1318 - val_loss: 32.1107 - val_mae: 3.9291\n",
            "Epoch 1004/1024\n",
            "404/404 [==============================] - 0s 99us/sample - loss: 27.7568 - mae: 4.0088 - val_loss: 32.0113 - val_mae: 3.9274\n",
            "Epoch 1005/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 31.4933 - mae: 4.2273 - val_loss: 31.3837 - val_mae: 3.8843\n",
            "Epoch 1006/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 30.6661 - mae: 4.2186 - val_loss: 32.1548 - val_mae: 3.9655\n",
            "Epoch 1007/1024\n",
            "404/404 [==============================] - 0s 91us/sample - loss: 31.4230 - mae: 4.2705 - val_loss: 34.7837 - val_mae: 4.2182\n",
            "Epoch 1008/1024\n",
            "404/404 [==============================] - 0s 111us/sample - loss: 29.3319 - mae: 4.1497 - val_loss: 38.2008 - val_mae: 4.5308\n",
            "Epoch 1009/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 30.4803 - mae: 4.0020 - val_loss: 37.9606 - val_mae: 4.5126\n",
            "Epoch 1010/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 29.7331 - mae: 4.1767 - val_loss: 34.7956 - val_mae: 4.2266\n",
            "Epoch 1011/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 31.4357 - mae: 4.2058 - val_loss: 31.5686 - val_mae: 3.9328\n",
            "Epoch 1012/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 33.6261 - mae: 4.3198 - val_loss: 30.8779 - val_mae: 3.8724\n",
            "Epoch 1013/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 26.5936 - mae: 4.0045 - val_loss: 31.3112 - val_mae: 3.8934\n",
            "Epoch 1014/1024\n",
            "404/404 [==============================] - 0s 90us/sample - loss: 29.8883 - mae: 4.1443 - val_loss: 33.7497 - val_mae: 4.1075\n",
            "Epoch 1015/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 26.6457 - mae: 3.8240 - val_loss: 36.4719 - val_mae: 4.3638\n",
            "Epoch 1016/1024\n",
            "404/404 [==============================] - 0s 100us/sample - loss: 33.1251 - mae: 4.2047 - val_loss: 35.3161 - val_mae: 4.2597\n",
            "Epoch 1017/1024\n",
            "404/404 [==============================] - 0s 97us/sample - loss: 27.3292 - mae: 4.0446 - val_loss: 33.2410 - val_mae: 4.0640\n",
            "Epoch 1018/1024\n",
            "404/404 [==============================] - 0s 98us/sample - loss: 23.8391 - mae: 3.7102 - val_loss: 31.6943 - val_mae: 3.9207\n",
            "Epoch 1019/1024\n",
            "404/404 [==============================] - 0s 110us/sample - loss: 29.8884 - mae: 4.0918 - val_loss: 31.4854 - val_mae: 3.9054\n",
            "Epoch 1020/1024\n",
            "404/404 [==============================] - 0s 101us/sample - loss: 31.6417 - mae: 4.2671 - val_loss: 33.6648 - val_mae: 4.1233\n",
            "Epoch 1021/1024\n",
            "404/404 [==============================] - 0s 95us/sample - loss: 29.2462 - mae: 4.1194 - val_loss: 38.5729 - val_mae: 4.5840\n",
            "Epoch 1022/1024\n",
            "404/404 [==============================] - 0s 112us/sample - loss: 31.7979 - mae: 4.3139 - val_loss: 39.7421 - val_mae: 4.6844\n",
            "Epoch 1023/1024\n",
            "404/404 [==============================] - 0s 96us/sample - loss: 34.6811 - mae: 4.2631 - val_loss: 35.8362 - val_mae: 4.3254\n",
            "Epoch 1024/1024\n",
            "404/404 [==============================] - 0s 93us/sample - loss: 30.5749 - mae: 4.0901 - val_loss: 32.8489 - val_mae: 4.0272\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JyG3bXr_sbl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4ea7c836-7dc0-419b-a01e-b42d3cfca298"
      },
      "source": [
        "madCowintheCah = moodel.evaluate( xTest, yTest, verbose= 1)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "102/102 [==============================] - 0s 152us/sample - loss: 32.8489 - mae: 4.0272\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjd9PRfEPSjv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "outputId": "14352f8a-8322-4ffc-b800-3ebd3b09619d"
      },
      "source": [
        "moo = go.Figure()\n",
        "moo.add_trace( go.Scattergl( y= mooFit.history[ 'loss'],\n",
        "                             name= 'Train'))\n",
        "\n",
        "moo.add_trace( go.Scattergl( y= mooFit.history[ 'val_loss'],\n",
        "                             name= 'Valid'))\n",
        "\n",
        "\n",
        "moo.update_layout( height= 720, width= 960,\n",
        "                   xaxis_title= 'Epoch',\n",
        "                   yaxis_title= 'Loss'\n",
        ") \n",
        "\n",
        "moo.show()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"adbd32ea-0201-4254-90d4-e82e55732cac\" class=\"plotly-graph-div\" style=\"height:720px; width:960px;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"adbd32ea-0201-4254-90d4-e82e55732cac\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'adbd32ea-0201-4254-90d4-e82e55732cac',\n",
              "                        [{\"name\": \"Train\", \"type\": \"scattergl\", \"y\": [1620.3861083984375, 9019.6083984375, 1640.4322509765625, 2181.31396484375, 4211.69775390625, 3010.447021484375, 1446.1011962890625, 627.3757934570312, 532.5693969726562, 886.953369140625, 990.999755859375, 1111.19140625, 818.2447509765625, 543.54541015625, 425.1577453613281, 312.7974853515625, 278.01513671875, 282.18414306640625, 288.4034423828125, 326.7229309082031, 300.41357421875, 317.81427001953125, 346.1587219238281, 326.93707275390625, 310.7384948730469, 302.3000183105469, 232.25909423828125, 225.49710083007812, 217.95797729492188, 162.1252899169922, 161.33827209472656, 155.531494140625, 148.0756378173828, 143.47816467285156, 144.40562438964844, 136.89337158203125, 157.25291442871094, 162.51641845703125, 151.02113342285156, 154.64419555664062, 153.10243225097656, 146.37229919433594, 162.33120727539062, 127.00242614746094, 130.12139892578125, 118.665283203125, 125.26338195800781, 132.95484924316406, 126.85565948486328, 121.62491607666016, 131.4572296142578, 128.3983917236328, 106.91267395019531, 126.0018081665039, 125.06907653808594, 114.58684539794922, 121.26114654541016, 127.38090515136719, 108.71589660644531, 122.72816467285156, 111.40739440917969, 109.96326446533203, 105.94111633300781, 113.21166229248047, 105.9996337890625, 120.1369400024414, 119.77854919433594, 114.7502670288086, 108.19942474365234, 114.41728210449219, 114.27922058105469, 99.16873168945312, 115.31127166748047, 109.22107696533203, 108.94134521484375, 103.01789855957031, 113.59795379638672, 96.47512817382812, 103.37571716308594, 104.50755310058594, 102.03211212158203, 95.8917465209961, 101.78608703613281, 102.1574478149414, 117.06462097167969, 105.59622192382812, 103.67619323730469, 106.26689910888672, 102.28453826904297, 95.6104736328125, 93.88306427001953, 95.87062072753906, 98.86522674560547, 93.78585052490234, 87.5834732055664, 86.09248352050781, 93.85875701904297, 100.58739471435547, 102.58466339111328, 103.32182312011719, 105.74011993408203, 98.60163879394531, 117.08856964111328, 95.3997802734375, 102.744384765625, 107.78884887695312, 97.18710327148438, 93.40728759765625, 97.67057800292969, 97.87821197509766, 98.50210571289062, 99.58781433105469, 89.914306640625, 89.02658081054688, 90.16050720214844, 89.81683349609375, 96.337890625, 90.23979187011719, 80.99747467041016, 87.84265899658203, 89.3460693359375, 95.39070129394531, 81.52127838134766, 98.23393249511719, 92.46987915039062, 94.18099975585938, 93.72460174560547, 87.57331085205078, 95.69082641601562, 97.81636047363281, 93.45257568359375, 88.90909576416016, 98.63316345214844, 93.55738830566406, 84.4432144165039, 87.04017639160156, 93.68822479248047, 96.9980239868164, 85.46446990966797, 87.5585708618164, 87.1944351196289, 84.80048370361328, 86.62374114990234, 88.2765121459961, 95.79486083984375, 83.87126159667969, 89.0323257446289, 87.57665252685547, 86.87569427490234, 79.48863220214844, 83.65733337402344, 82.66780090332031, 94.0861587524414, 89.55884552001953, 86.14876556396484, 86.63921356201172, 85.03315734863281, 80.15943908691406, 84.42611694335938, 84.14047241210938, 89.4542007446289, 84.7532958984375, 86.55882263183594, 89.65625762939453, 77.28157043457031, 85.86834716796875, 82.44940948486328, 83.01070404052734, 89.3213882446289, 84.54354858398438, 85.36078643798828, 81.37354278564453, 84.00845336914062, 81.54517364501953, 78.29656219482422, 77.406982421875, 91.7685317993164, 77.51200103759766, 89.1998291015625, 88.56659698486328, 77.3333969116211, 72.94647979736328, 86.24373626708984, 83.51386260986328, 82.38184356689453, 74.37582397460938, 79.99193572998047, 78.78883361816406, 90.1401596069336, 82.1602554321289, 87.20848846435547, 81.4318618774414, 88.55917358398438, 80.6382064819336, 81.2639389038086, 80.61345672607422, 76.12055206298828, 78.40568542480469, 80.60087585449219, 79.314208984375, 76.81907653808594, 84.27980041503906, 73.34809112548828, 77.76774597167969, 86.59899139404297, 77.4136734008789, 78.12012481689453, 80.49774932861328, 71.1919937133789, 86.4532241821289, 80.56031799316406, 77.7652587890625, 78.66270446777344, 74.45118713378906, 71.13874053955078, 82.51033782958984, 86.97303009033203, 80.48890686035156, 84.48292541503906, 70.56436920166016, 75.46846771240234, 78.9499740600586, 77.252197265625, 71.94062805175781, 77.9832534790039, 78.09461975097656, 78.17986297607422, 81.61509704589844, 70.2486572265625, 78.19384002685547, 80.89216613769531, 70.2901611328125, 69.68221282958984, 67.73099517822266, 77.56320190429688, 67.81378936767578, 78.13636779785156, 76.77973937988281, 73.6875991821289, 78.52381134033203, 78.36063385009766, 74.43575286865234, 71.52268981933594, 74.15213775634766, 71.67633819580078, 78.14757537841797, 79.57908630371094, 78.78266143798828, 70.64087677001953, 70.33151245117188, 71.11497497558594, 78.58536529541016, 79.66136169433594, 67.56587219238281, 75.50251770019531, 75.95574188232422, 75.88570404052734, 64.53816986083984, 79.79560089111328, 81.01811218261719, 71.79737854003906, 73.41151428222656, 72.91333770751953, 72.19810485839844, 74.78287506103516, 72.85694122314453, 67.69732666015625, 67.7066879272461, 74.50911712646484, 71.09584045410156, 74.01646423339844, 71.47261810302734, 67.98854064941406, 68.23847198486328, 67.10594940185547, 67.99372100830078, 68.28206634521484, 66.75391387939453, 75.92235565185547, 76.89964294433594, 72.04464721679688, 68.5457534790039, 66.81910705566406, 75.64978790283203, 76.70137786865234, 64.15377807617188, 63.266876220703125, 59.853084564208984, 66.6409683227539, 68.44052124023438, 70.78849029541016, 68.2132568359375, 63.33488464355469, 60.3140983581543, 69.27760314941406, 67.9572525024414, 66.09904479980469, 65.89616394042969, 64.82344818115234, 67.58649444580078, 64.68279266357422, 63.23387145996094, 69.85008239746094, 57.1402473449707, 62.00341796875, 57.098060607910156, 75.8355712890625, 62.11383056640625, 66.92266082763672, 62.823238372802734, 62.91135025024414, 59.2179069519043, 61.090370178222656, 62.603458404541016, 59.857948303222656, 65.30160522460938, 61.4318733215332, 62.6120719909668, 66.2874526977539, 61.12049102783203, 63.31112289428711, 58.73714065551758, 63.677207946777344, 60.27163314819336, 56.51806640625, 56.4759635925293, 55.947052001953125, 63.08454513549805, 62.47420883178711, 58.741905212402344, 59.320011138916016, 69.04657745361328, 63.362159729003906, 58.99565887451172, 55.26806640625, 60.39888381958008, 61.36252212524414, 59.947017669677734, 56.79029083251953, 60.78532409667969, 65.33793640136719, 59.128726959228516, 53.85675048828125, 57.68370056152344, 63.531593322753906, 60.822879791259766, 53.455841064453125, 56.93818664550781, 57.89330291748047, 55.911808013916016, 54.223331451416016, 56.56720733642578, 59.96626663208008, 56.128997802734375, 58.41898727416992, 58.083091735839844, 59.667301177978516, 49.74150466918945, 57.065433502197266, 59.51926040649414, 58.32448959350586, 68.77523803710938, 52.60144805908203, 50.50289535522461, 51.719905853271484, 54.84568786621094, 57.55490493774414, 59.13901138305664, 58.83232116699219, 67.76817321777344, 56.9995231628418, 59.019989013671875, 53.350276947021484, 52.70462417602539, 53.683876037597656, 54.61845779418945, 56.60295486450195, 55.64733123779297, 51.75111770629883, 50.80696487426758, 57.43659210205078, 52.03654479980469, 55.01382827758789, 53.7797737121582, 60.0509147644043, 55.07285690307617, 57.06357192993164, 54.521644592285156, 50.81635284423828, 55.6126823425293, 54.682838439941406, 49.09910202026367, 50.35417938232422, 49.410457611083984, 52.81718826293945, 53.79167938232422, 49.60943603515625, 57.05877685546875, 53.81332778930664, 55.3973503112793, 60.71750259399414, 48.11357116699219, 51.29278564453125, 50.36006546020508, 45.390357971191406, 55.72037887573242, 48.34208297729492, 51.08687210083008, 50.81957244873047, 56.31760025024414, 55.4961051940918, 48.222694396972656, 54.56814193725586, 53.27553176879883, 45.965003967285156, 48.051063537597656, 51.171966552734375, 52.518951416015625, 60.76260757446289, 55.47174835205078, 50.950462341308594, 49.94449234008789, 49.90272521972656, 48.86153030395508, 50.42789077758789, 47.20274353027344, 50.11941146850586, 52.441436767578125, 47.76038360595703, 51.6407585144043, 44.97943878173828, 52.545989990234375, 45.82465744018555, 44.00545883178711, 51.247371673583984, 49.8349494934082, 47.89464569091797, 47.01237106323242, 50.99154281616211, 47.887638092041016, 42.407135009765625, 51.99555206298828, 50.971649169921875, 48.64301300048828, 51.287254333496094, 50.92593765258789, 43.81822967529297, 43.72768020629883, 48.254554748535156, 47.84955978393555, 44.21920394897461, 48.068389892578125, 47.34970474243164, 52.01887893676758, 43.53159713745117, 45.029640197753906, 47.48712158203125, 48.015541076660156, 47.35997772216797, 46.88167953491211, 42.12176513671875, 43.82810974121094, 41.81825256347656, 45.64286804199219, 50.188873291015625, 50.2659912109375, 44.1628303527832, 43.0866813659668, 52.372562408447266, 46.7368049621582, 47.74985885620117, 39.127220153808594, 46.59749984741211, 45.65810775756836, 44.75810241699219, 39.71147155761719, 46.337093353271484, 43.71290588378906, 44.72573471069336, 48.14300537109375, 42.64516830444336, 43.534828186035156, 46.21720886230469, 49.63371658325195, 52.273033142089844, 44.3713493347168, 47.26697540283203, 45.43871307373047, 47.13420867919922, 43.78901290893555, 46.195960998535156, 44.351261138916016, 42.40855026245117, 41.99074172973633, 45.58391571044922, 40.79557800292969, 45.79423522949219, 48.1333122253418, 44.72054672241211, 45.68733215332031, 42.541481018066406, 50.01724624633789, 42.35239028930664, 41.58881378173828, 45.94083023071289, 39.963199615478516, 45.44733810424805, 42.8319091796875, 45.85334014892578, 44.2003173828125, 46.52587890625, 37.64986038208008, 43.77670669555664, 41.34751510620117, 43.54011154174805, 47.08638381958008, 46.066932678222656, 44.4819450378418, 50.035865783691406, 40.94710922241211, 39.72830581665039, 43.0939826965332, 46.517051696777344, 47.40434646606445, 43.287879943847656, 43.48735809326172, 41.89398193359375, 42.38386154174805, 44.721519470214844, 40.417335510253906, 47.615047454833984, 46.70376968383789, 46.17679977416992, 42.38544845581055, 47.134735107421875, 43.69880294799805, 47.376060485839844, 37.09280776977539, 40.93775939941406, 39.55553436279297, 40.72240447998047, 46.272926330566406, 37.28108596801758, 39.28596115112305, 42.56291198730469, 41.89107131958008, 39.563697814941406, 38.902626037597656, 40.35582733154297, 37.885032653808594, 45.49132537841797, 42.336246490478516, 38.44403076171875, 39.517372131347656, 36.73548889160156, 39.656822204589844, 38.0071907043457, 42.700897216796875, 42.52423858642578, 38.5615119934082, 41.19499206542969, 47.21816635131836, 40.53541564941406, 44.45612716674805, 44.713417053222656, 41.78096389770508, 46.15800476074219, 40.047420501708984, 34.57795715332031, 43.219215393066406, 40.96927261352539, 38.80390167236328, 41.8290901184082, 39.00175094604492, 38.40935134887695, 41.29332733154297, 38.732906341552734, 46.559322357177734, 39.567020416259766, 40.94101333618164, 38.5894660949707, 30.86895751953125, 39.48208999633789, 42.939186096191406, 40.108177185058594, 42.1290283203125, 41.501922607421875, 42.26708984375, 39.65883255004883, 42.739009857177734, 39.50343322753906, 41.762149810791016, 44.20598220825195, 38.45772933959961, 40.4412727355957, 43.621456146240234, 41.684059143066406, 44.69331741333008, 37.9329833984375, 44.85799789428711, 40.904911041259766, 42.3592414855957, 37.310699462890625, 40.98324966430664, 35.972084045410156, 37.236785888671875, 36.272972106933594, 36.93381118774414, 42.279998779296875, 38.8703727722168, 41.87814712524414, 37.36127853393555, 39.43561553955078, 39.5684814453125, 43.979034423828125, 40.932098388671875, 42.562461853027344, 40.311195373535156, 34.50385665893555, 38.8665657043457, 43.15697479248047, 43.19252395629883, 40.854007720947266, 42.69097900390625, 38.68402862548828, 43.24863052368164, 36.84752655029297, 39.649112701416016, 34.9646110534668, 36.858436584472656, 33.273284912109375, 38.989501953125, 39.28137969970703, 37.437103271484375, 40.73505783081055, 38.844146728515625, 40.20808792114258, 37.106727600097656, 33.36019515991211, 40.500614166259766, 35.76653289794922, 36.192657470703125, 37.01032638549805, 34.74012756347656, 38.18268585205078, 38.152408599853516, 36.99888229370117, 37.37739181518555, 39.39301681518555, 33.44227981567383, 40.827545166015625, 43.277610778808594, 34.846893310546875, 40.804954528808594, 30.979061126708984, 34.030677795410156, 40.580406188964844, 37.31228256225586, 36.602848052978516, 41.363861083984375, 37.307247161865234, 37.437469482421875, 39.283111572265625, 37.91635513305664, 37.66944122314453, 34.19301223754883, 44.755496978759766, 38.6319465637207, 37.95614242553711, 35.0561408996582, 40.16518020629883, 34.90681076049805, 38.16611099243164, 36.176979064941406, 38.99740219116211, 35.803897857666016, 37.996604919433594, 35.46238708496094, 33.525550842285156, 35.68596649169922, 37.530967712402344, 37.28002166748047, 34.456748962402344, 38.697425842285156, 39.6292839050293, 32.2658576965332, 37.596431732177734, 37.9643440246582, 36.321754455566406, 36.51240539550781, 34.77894592285156, 39.14101028442383, 35.259883880615234, 34.343605041503906, 33.14766311645508, 34.542205810546875, 40.865203857421875, 38.312957763671875, 31.426467895507812, 32.8116569519043, 32.08417892456055, 35.366512298583984, 34.852378845214844, 38.9357795715332, 37.61064910888672, 36.44670486450195, 37.9007453918457, 35.55996322631836, 39.521785736083984, 36.008384704589844, 36.86620330810547, 37.144493103027344, 35.04145431518555, 34.25634765625, 38.026268005371094, 40.2625617980957, 32.10788345336914, 38.988948822021484, 30.522233963012695, 33.971492767333984, 33.35561752319336, 37.110538482666016, 37.86511993408203, 34.46506118774414, 35.158634185791016, 35.90940475463867, 34.51785659790039, 33.976600646972656, 35.341270446777344, 34.33566665649414, 37.19358825683594, 30.18160629272461, 39.8888053894043, 36.5688362121582, 32.93906784057617, 29.384510040283203, 36.4193000793457, 40.2136344909668, 34.83442687988281, 35.80183410644531, 38.38481140136719, 35.56498336791992, 34.60460662841797, 37.23240661621094, 32.31535720825195, 36.42343521118164, 37.37958526611328, 36.18181610107422, 36.473304748535156, 36.036075592041016, 32.44440460205078, 32.861576080322266, 33.786312103271484, 31.34512710571289, 33.72708511352539, 38.2637825012207, 36.839027404785156, 37.76427459716797, 31.186073303222656, 32.85875701904297, 35.72883987426758, 36.75279998779297, 38.02334976196289, 35.78411102294922, 35.45111846923828, 37.057518005371094, 36.96812057495117, 35.15388870239258, 33.11825180053711, 34.06755447387695, 29.56134033203125, 35.7557373046875, 31.57343864440918, 32.300106048583984, 36.693145751953125, 31.36007308959961, 35.147212982177734, 32.79581832885742, 34.819095611572266, 37.70024490356445, 31.345367431640625, 35.26475143432617, 36.854591369628906, 37.19600296020508, 36.34796142578125, 39.9454231262207, 35.651756286621094, 31.6699161529541, 33.708168029785156, 33.556373596191406, 30.17034339904785, 33.20173263549805, 38.77021026611328, 32.81901168823242, 31.89596939086914, 41.13404846191406, 37.1553840637207, 31.725175857543945, 37.348960876464844, 31.255096435546875, 33.810577392578125, 36.212684631347656, 37.216033935546875, 37.5829963684082, 35.8739013671875, 38.3775634765625, 36.42782974243164, 34.327491760253906, 33.96014404296875, 33.930633544921875, 34.37942123413086, 32.690067291259766, 34.57918930053711, 37.373844146728516, 33.83680725097656, 31.04114532470703, 32.1953125, 38.75985336303711, 41.182125091552734, 34.20756912231445, 33.788150787353516, 31.19095230102539, 30.885496139526367, 35.81931686401367, 37.92621994018555, 33.28035354614258, 32.68490982055664, 35.88175964355469, 30.965238571166992, 33.69269561767578, 39.593528747558594, 31.331987380981445, 36.46055603027344, 32.38069152832031, 35.06542205810547, 34.24297332763672, 31.686416625976562, 32.17375564575195, 31.0251522064209, 38.13869857788086, 30.52025604248047, 31.572174072265625, 31.12298583984375, 33.875213623046875, 32.19961166381836, 32.964603424072266, 30.997886657714844, 32.09910202026367, 30.650814056396484, 33.62171173095703, 35.15808868408203, 34.73317337036133, 36.1060791015625, 30.231672286987305, 37.793338775634766, 31.863473892211914, 28.742149353027344, 32.735958099365234, 35.144287109375, 32.310150146484375, 29.695798873901367, 28.141084671020508, 36.8557014465332, 31.288482666015625, 35.150787353515625, 33.102134704589844, 36.56908416748047, 29.974882125854492, 35.8041877746582, 28.76692771911621, 29.44037628173828, 30.669769287109375, 32.8305549621582, 33.769989013671875, 31.358875274658203, 32.14399719238281, 33.28685760498047, 27.8847713470459, 34.84680938720703, 32.79300308227539, 32.02216720581055, 28.591171264648438, 30.268465042114258, 32.56974792480469, 31.184776306152344, 30.571964263916016, 33.914634704589844, 30.929121017456055, 26.1965389251709, 30.641845703125, 31.68886375427246, 32.9683837890625, 30.09417724609375, 26.322429656982422, 30.27324104309082, 32.79458999633789, 30.76812171936035, 29.401628494262695, 33.032958984375, 34.346405029296875, 34.02827835083008, 32.974143981933594, 33.00674057006836, 30.118511199951172, 33.51748275756836, 30.613561630249023, 28.494300842285156, 30.70612907409668, 28.564594268798828, 33.00719451904297, 32.50719451904297, 31.87022590637207, 32.98567581176758, 36.436256408691406, 31.129396438598633, 29.74226188659668, 34.54350662231445, 35.34642028808594, 33.99103927612305, 36.282955169677734, 31.87416648864746, 28.050844192504883, 31.370874404907227, 34.96321105957031, 29.22970199584961, 29.983938217163086, 32.601558685302734, 32.10055923461914, 28.895631790161133, 31.16672134399414, 32.6019172668457, 28.546606063842773, 36.783966064453125, 34.86813735961914, 31.257667541503906, 26.86076545715332, 34.706260681152344, 31.570270538330078, 35.08225631713867, 30.351463317871094, 30.105070114135742, 35.13127899169922, 32.81439208984375, 31.546932220458984, 33.5858039855957, 35.86560821533203, 33.05506134033203, 32.12534713745117, 32.276119232177734, 31.846263885498047, 35.08237075805664, 31.446088790893555, 34.50139617919922, 32.38602066040039, 30.682403564453125, 37.19822692871094, 35.85886001586914, 33.415733337402344, 34.4961051940918, 31.031518936157227, 39.2329216003418, 31.115652084350586, 26.99873161315918, 32.95417022705078, 33.122100830078125, 28.950780868530273, 33.30632781982422, 35.117698669433594, 31.16270637512207, 29.87147331237793, 29.489639282226562, 30.43252944946289, 30.58834457397461, 33.600040435791016, 28.983179092407227, 33.187686920166016, 30.84073829650879, 33.093448638916016, 33.1646842956543, 32.435150146484375, 29.166296005249023, 32.45669174194336, 29.635696411132812, 30.39491844177246, 34.065162658691406, 31.365882873535156, 31.887550354003906, 32.51314163208008, 27.741905212402344, 35.31459426879883, 31.283716201782227, 30.7172794342041, 31.171939849853516, 29.305400848388672, 26.009294509887695, 37.47682189941406, 26.558605194091797, 33.55595779418945, 31.86394691467285, 30.698829650878906, 29.913034439086914, 29.19109535217285, 28.683521270751953, 27.756811141967773, 31.493330001831055, 30.666135787963867, 31.42301368713379, 29.3319034576416, 30.48027229309082, 29.73314666748047, 31.435749053955078, 33.626060485839844, 26.59356689453125, 29.888263702392578, 26.64565086364746, 33.125091552734375, 27.329185485839844, 23.83914566040039, 29.88837432861328, 31.641672134399414, 29.246166229248047, 31.79789924621582, 34.68110275268555, 30.574880599975586]}, {\"name\": \"Valid\", \"type\": \"scattergl\", \"y\": [7839.0869140625, 706.4378051757812, 1246.4947509765625, 3115.828125, 2175.784423828125, 795.3259887695312, 183.13116455078125, 167.40415954589844, 315.2185363769531, 382.1657409667969, 338.8251647949219, 231.6734619140625, 147.36172485351562, 138.0733184814453, 195.9430389404297, 282.7724609375, 367.371337890625, 439.33062744140625, 492.62908935546875, 524.461181640625, 536.6572265625, 532.4386596679688, 516.785400390625, 491.6346435546875, 458.4388427734375, 418.7991027832031, 377.943359375, 337.02691650390625, 296.37127685546875, 258.8835754394531, 224.71453857421875, 194.51072692871094, 170.05453491210938, 150.7774200439453, 137.06411743164062, 128.21328735351562, 123.45448303222656, 122.47773742675781, 125.19525146484375, 131.54002380371094, 141.11585998535156, 152.61155700683594, 166.0536651611328, 180.84805297851562, 197.03582763671875, 213.37753295898438, 229.73277282714844, 244.8809356689453, 258.39178466796875, 268.6541748046875, 275.7596740722656, 279.094482421875, 279.32940673828125, 277.05322265625, 271.77777099609375, 264.5538024902344, 256.5214538574219, 247.35458374023438, 237.56605529785156, 228.02687072753906, 219.9640655517578, 213.4589385986328, 207.3898162841797, 203.15675354003906, 199.7679443359375, 198.27316284179688, 199.47813415527344, 202.33023071289062, 206.076171875, 209.73529052734375, 212.9463348388672, 215.8057861328125, 218.2904815673828, 219.8118133544922, 220.57980346679688, 220.94776916503906, 220.40721130371094, 220.4119110107422, 219.61065673828125, 219.23292541503906, 219.25543212890625, 218.5350341796875, 217.26812744140625, 215.439697265625, 213.8727264404297, 211.56192016601562, 209.5982666015625, 207.7802734375, 204.7270965576172, 202.20274353027344, 199.3636474609375, 196.8390350341797, 194.1985321044922, 191.81451416015625, 190.18345642089844, 187.6493377685547, 186.3236846923828, 185.1661834716797, 184.54373168945312, 182.641845703125, 181.9684600830078, 181.6639404296875, 183.02085876464844, 184.9461669921875, 186.2287139892578, 187.3378448486328, 186.89378356933594, 186.4485626220703, 184.0702362060547, 180.44012451171875, 176.3416748046875, 172.546630859375, 168.04214477539062, 163.55996704101562, 160.10365295410156, 158.10536193847656, 156.6062469482422, 156.55995178222656, 157.09144592285156, 159.3903045654297, 163.37005615234375, 167.02078247070312, 168.90919494628906, 169.08370971679688, 166.0972137451172, 163.05226135253906, 160.42440795898438, 158.26727294921875, 155.47344970703125, 152.7209930419922, 151.1708221435547, 150.01828002929688, 149.60336303710938, 150.04490661621094, 149.9884490966797, 150.41648864746094, 151.56375122070312, 151.58241271972656, 151.3485870361328, 150.60572814941406, 148.67727661132812, 145.8765106201172, 142.83163452148438, 139.30593872070312, 137.02734375, 135.6298065185547, 135.8025360107422, 137.2827606201172, 138.96151733398438, 140.0623779296875, 140.1169891357422, 139.09519958496094, 137.70925903320312, 136.0103759765625, 132.61892700195312, 130.6242218017578, 130.07818603515625, 129.64508056640625, 130.72918701171875, 130.81202697753906, 130.57472229003906, 130.60523986816406, 131.4503631591797, 133.24266052246094, 133.53836059570312, 131.6194610595703, 128.9217071533203, 125.80687713623047, 123.96150970458984, 122.6441421508789, 122.12635803222656, 123.57743835449219, 125.95203399658203, 127.71867370605469, 129.3491668701172, 130.81451416015625, 130.96142578125, 130.00238037109375, 127.82444763183594, 124.7502670288086, 121.00830841064453, 116.98017883300781, 114.14444732666016, 112.42144775390625, 112.30447387695312, 113.3737564086914, 116.6335678100586, 120.0013656616211, 123.67888641357422, 126.80438232421875, 127.88338470458984, 127.64966583251953, 125.04623413085938, 120.24034118652344, 116.12181091308594, 113.16136932373047, 110.62552642822266, 109.08259582519531, 109.0029067993164, 113.13546752929688, 117.89783477783203, 121.27012634277344, 124.11565399169922, 125.60115814208984, 125.26692962646484, 122.24774169921875, 116.39837646484375, 109.4567642211914, 103.38821411132812, 99.54026794433594, 101.0132827758789, 104.85979461669922, 110.8946304321289, 115.09406280517578, 117.32982635498047, 118.22051239013672, 116.16578674316406, 112.34197235107422, 107.71397399902344, 102.5688705444336, 98.07882690429688, 98.68258666992188, 101.02005004882812, 104.05902099609375, 106.97633361816406, 109.18295288085938, 111.90827178955078, 112.27742004394531, 111.80558776855469, 108.75438690185547, 104.9056396484375, 99.58604431152344, 94.05089569091797, 89.8155517578125, 88.1760025024414, 87.97562408447266, 91.43667602539062, 95.39038848876953, 98.48912048339844, 100.19174194335938, 99.70763397216797, 98.25310516357422, 96.5001449584961, 95.1673355102539, 93.95034790039062, 93.2475814819336, 93.06475067138672, 94.63449096679688, 95.6563720703125, 96.12998962402344, 95.06089782714844, 93.04995727539062, 91.67304992675781, 91.06163024902344, 90.85462951660156, 91.05315399169922, 89.72280883789062, 88.6148910522461, 89.22066497802734, 89.7510757446289, 89.43709564208984, 89.37722778320312, 91.66777801513672, 94.21438598632812, 94.2530517578125, 92.50759887695312, 88.76029205322266, 85.40650177001953, 83.16699981689453, 82.9705581665039, 84.10862731933594, 87.1657485961914, 90.84052276611328, 92.55681610107422, 91.3664321899414, 87.55728912353516, 81.47486114501953, 77.25785827636719, 76.3387451171875, 78.23248291015625, 83.4960708618164, 88.47135162353516, 89.6735610961914, 89.40789794921875, 85.30753326416016, 82.24365997314453, 80.78335571289062, 80.19368743896484, 80.16805267333984, 81.65953063964844, 82.5269775390625, 82.07440185546875, 80.68431091308594, 80.99226379394531, 83.9975357055664, 86.39051055908203, 83.51374816894531, 79.30108642578125, 77.77873992919922, 77.6879653930664, 79.41777038574219, 78.78890991210938, 78.35658264160156, 78.62127685546875, 79.79904174804688, 81.33831024169922, 80.14590454101562, 77.64289093017578, 74.57524871826172, 73.65474700927734, 75.40142059326172, 79.4260482788086, 80.55693054199219, 79.1998291015625, 76.24407196044922, 72.14310455322266, 68.64915466308594, 68.4582748413086, 72.40879821777344, 78.7134017944336, 83.87470245361328, 86.50833892822266, 84.42379760742188, 79.49861907958984, 74.41027069091797, 69.39594268798828, 66.52091217041016, 67.22871398925781, 70.20285034179688, 75.28446960449219, 77.2989730834961, 77.16719055175781, 72.57794952392578, 70.26168823242188, 68.4610366821289, 69.7735824584961, 73.1263198852539, 75.9751968383789, 76.50379943847656, 76.0994873046875, 71.8049545288086, 68.361572265625, 67.1777114868164, 67.7945785522461, 70.34957885742188, 72.6115951538086, 73.02312469482422, 70.09184265136719, 64.48932647705078, 61.126121520996094, 63.07125473022461, 67.53296661376953, 72.30523681640625, 71.9471435546875, 64.62625122070312, 59.39952850341797, 61.04539489746094, 64.57137298583984, 70.14108276367188, 75.68389892578125, 75.1573257446289, 70.82298278808594, 62.721073150634766, 57.18077087402344, 55.656307220458984, 57.669532775878906, 62.593997955322266, 68.74080657958984, 72.28056335449219, 70.3362808227539, 65.09687805175781, 61.20881652832031, 58.649147033691406, 58.50666046142578, 60.890533447265625, 62.633323669433594, 61.80362319946289, 61.460086822509766, 61.50481414794922, 63.723846435546875, 62.751949310302734, 62.672149658203125, 63.0875358581543, 61.87190628051758, 61.06642150878906, 57.5809211730957, 57.72306823730469, 61.506385803222656, 64.58714294433594, 65.5655746459961, 64.38423156738281, 62.387786865234375, 60.57337951660156, 59.37076187133789, 58.29515075683594, 54.4524040222168, 53.71993637084961, 56.14033126831055, 62.01427459716797, 67.41230773925781, 68.44451904296875, 65.26961517333984, 58.74365234375, 56.13487243652344, 54.380889892578125, 55.458065032958984, 61.12284469604492, 68.97127532958984, 70.73701477050781, 67.95142364501953, 58.6165657043457, 53.33121109008789, 51.95716857910156, 55.50794982910156, 60.52962112426758, 63.852386474609375, 66.37313079833984, 63.865482330322266, 58.74864196777344, 52.483978271484375, 49.664520263671875, 52.87130355834961, 56.4165153503418, 59.96675491333008, 59.41078186035156, 59.47163009643555, 62.00102615356445, 64.69646453857422, 63.87567901611328, 60.461463928222656, 56.572235107421875, 53.09648895263672, 50.13938522338867, 51.094451904296875, 54.26355743408203, 58.606204986572266, 60.683868408203125, 61.25181579589844, 56.6611328125, 52.543800354003906, 48.3265495300293, 46.293922424316406, 49.7219123840332, 56.219573974609375, 60.387821197509766, 57.97154998779297, 53.31918716430664, 48.45891189575195, 50.05538558959961, 54.198726654052734, 57.286869049072266, 58.06633377075195, 55.80291748046875, 52.67519760131836, 50.393043518066406, 49.13382339477539, 49.29964828491211, 49.12411880493164, 49.66827392578125, 51.99281311035156, 53.36027526855469, 55.35710906982422, 55.77927780151367, 51.90068817138672, 50.99890899658203, 51.97066116333008, 52.269920349121094, 55.5911979675293, 58.81855392456055, 58.87467575073242, 55.998680114746094, 53.43924331665039, 51.018096923828125, 48.15071487426758, 50.48991012573242, 54.25044631958008, 59.07969665527344, 62.616920471191406, 58.93010711669922, 49.76352310180664, 43.376686096191406, 42.03117370605469, 45.49385452270508, 55.37274932861328, 60.470001220703125, 59.03081512451172, 53.11326217651367, 48.13671875, 44.79881286621094, 46.32455062866211, 52.38859176635742, 62.11671829223633, 64.1351547241211, 58.53974151611328, 49.06898498535156, 45.32384490966797, 46.17631912231445, 50.08076858520508, 52.96687698364258, 54.99141311645508, 55.43394470214844, 54.517581939697266, 52.99123001098633, 50.894588470458984, 49.53717803955078, 50.52627182006836, 54.46302795410156, 54.314300537109375, 50.75730514526367, 47.880001068115234, 47.947113037109375, 52.33113479614258, 55.01841735839844, 53.68111801147461, 53.3636589050293, 51.42063903808594, 50.40484619140625, 46.911865234375, 47.909446716308594, 49.60283279418945, 51.011104583740234, 51.647064208984375, 52.65485382080078, 53.3132438659668, 48.75699234008789, 45.202415466308594, 43.49700927734375, 44.484222412109375, 49.27531051635742, 51.96407699584961, 55.4137077331543, 51.58537292480469, 49.02717971801758, 47.93327331542969, 49.4026985168457, 48.561500549316406, 47.22705078125, 47.61330032348633, 48.905330657958984, 48.61159133911133, 48.28583526611328, 48.052459716796875, 46.116661071777344, 43.662052154541016, 45.65892028808594, 49.411563873291016, 51.886993408203125, 51.72401428222656, 47.921417236328125, 40.93729782104492, 41.43879318237305, 45.79384994506836, 48.97594451904297, 48.1696662902832, 49.11162185668945, 47.060245513916016, 42.32582092285156, 41.82990264892578, 45.0885124206543, 49.21448516845703, 53.366703033447266, 53.0135612487793, 48.27743911743164, 41.310890197753906, 40.98894119262695, 44.361473083496094, 46.935279846191406, 50.738189697265625, 52.6171760559082, 48.482234954833984, 42.57538604736328, 40.21961975097656, 41.59492111206055, 44.625057220458984, 48.32542419433594, 51.45726013183594, 49.5897331237793, 45.25676727294922, 41.62832260131836, 40.06341552734375, 42.86612319946289, 46.32835006713867, 48.12815475463867, 49.08330535888672, 48.17764663696289, 45.06454849243164, 40.91385269165039, 39.601043701171875, 41.63154983520508, 48.29258728027344, 49.713748931884766, 46.00547790527344, 40.04048538208008, 38.267059326171875, 41.4630012512207, 47.03166961669922, 52.51292037963867, 52.60588073730469, 49.12345886230469, 42.66047668457031, 40.39566421508789, 43.01030349731445, 46.74573516845703, 48.56161117553711, 45.01496505737305, 44.865501403808594, 43.61310958862305, 40.99566650390625, 40.39858627319336, 42.661075592041016, 45.359615325927734, 48.28642654418945, 44.80656814575195, 40.85445022583008, 38.536739349365234, 40.04033660888672, 46.56089782714844, 51.387939453125, 48.78078079223633, 41.18648910522461, 37.19024658203125, 36.16461181640625, 38.48420715332031, 43.13640594482422, 46.94022750854492, 47.608150482177734, 47.52155685424805, 43.419410705566406, 39.69330978393555, 37.39430236816406, 37.91415023803711, 40.922306060791016, 46.423484802246094, 48.33740234375, 45.65049743652344, 40.548370361328125, 39.554954528808594, 38.16305160522461, 38.20280456542969, 42.106903076171875, 44.73015213012695, 44.47798538208008, 41.71539306640625, 41.78434371948242, 41.303653717041016, 41.15641403198242, 42.54035949707031, 43.060794830322266, 40.8646125793457, 39.62420654296875, 40.081382751464844, 44.45463943481445, 47.42469024658203, 45.8624382019043, 42.594383239746094, 40.037750244140625, 38.83429718017578, 40.17301559448242, 45.07691955566406, 47.393829345703125, 45.471397399902344, 41.62200164794922, 39.154563903808594, 39.7422981262207, 42.51780319213867, 43.542972564697266, 41.646026611328125, 38.946434020996094, 37.41569137573242, 38.57518005371094, 41.61905288696289, 44.6898307800293, 43.68022537231445, 38.917823791503906, 37.46795654296875, 38.031375885009766, 40.755882263183594, 43.42790603637695, 43.039581298828125, 41.752464294433594, 41.863922119140625, 41.389686584472656, 40.38627243041992, 41.48976135253906, 40.61928939819336, 40.67952346801758, 40.05876922607422, 37.1508674621582, 38.64974594116211, 41.415008544921875, 41.88418197631836, 42.939823150634766, 43.04701232910156, 41.938255310058594, 38.49858856201172, 37.65867233276367, 41.09385681152344, 44.16359329223633, 44.78593063354492, 41.20663070678711, 37.52489471435547, 35.78315353393555, 36.08219909667969, 41.951141357421875, 50.12825393676758, 50.434471130371094, 48.59379959106445, 40.91886901855469, 36.047000885009766, 35.28936004638672, 37.98006057739258, 44.142852783203125, 48.281959533691406, 48.02507781982422, 42.81502914428711, 37.19659423828125, 34.604984283447266, 36.07986831665039, 38.48064041137695, 42.43239212036133, 44.5568962097168, 42.76924514770508, 41.001182556152344, 38.02965545654297, 37.68557357788086, 39.48738479614258, 42.27296829223633, 43.88014221191406, 41.31942367553711, 39.48196029663086, 35.90220260620117, 36.129371643066406, 37.9461555480957, 42.39703369140625, 44.94290542602539, 43.53571701049805, 40.83977508544922, 38.64028549194336, 38.453765869140625, 38.44370651245117, 41.61324691772461, 43.95355224609375, 41.065574645996094, 39.9088134765625, 37.997127532958984, 36.09126281738281, 37.673343658447266, 39.964942932128906, 42.90074920654297, 42.090145111083984, 40.80744934082031, 38.3867073059082, 37.40966033935547, 37.465755462646484, 39.67677307128906, 40.678131103515625, 41.93986129760742, 41.548057556152344, 38.521759033203125, 34.568851470947266, 33.54585647583008, 34.712738037109375, 39.33941650390625, 44.477760314941406, 47.10102081298828, 42.62968826293945, 37.042640686035156, 34.236106872558594, 33.819419860839844, 35.28059387207031, 38.43470001220703, 40.257591247558594, 39.68643569946289, 38.15589904785156, 36.620269775390625, 35.299678802490234, 35.316707611083984, 37.385929107666016, 37.66886520385742, 38.28349685668945, 37.91722106933594, 37.010276794433594, 36.91313552856445, 36.623416900634766, 35.15958786010742, 37.537933349609375, 41.05134963989258, 43.031219482421875, 43.80571365356445, 42.49946212768555, 40.066341400146484, 38.79426574707031, 39.247501373291016, 39.91300964355469, 39.78509521484375, 36.9531364440918, 37.107215881347656, 37.38795471191406, 38.120601654052734, 39.872188568115234, 39.12254333496094, 37.550323486328125, 38.23097229003906, 38.470916748046875, 39.62990188598633, 42.67631149291992, 41.8403434753418, 41.4456901550293, 38.86934280395508, 35.37091064453125, 34.81505584716797, 37.380409240722656, 43.29655456542969, 46.07136535644531, 43.734771728515625, 38.65814208984375, 34.02314376831055, 34.32063674926758, 39.12113571166992, 45.98341369628906, 47.8758659362793, 43.30839920043945, 36.6054573059082, 33.789573669433594, 35.066925048828125, 37.569435119628906, 38.76241683959961, 41.176212310791016, 38.80675506591797, 34.979286193847656, 33.750667572021484, 33.62416458129883, 36.18472671508789, 42.44134521484375, 43.47798538208008, 40.01091384887695, 36.28407287597656, 34.832218170166016, 34.77410125732422, 37.06541442871094, 39.90620040893555, 40.44438552856445, 39.75723648071289, 37.58637237548828, 36.942230224609375, 36.16423797607422, 37.0474853515625, 38.48334884643555, 40.45248794555664, 39.354618072509766, 36.87445068359375, 34.284175872802734, 33.319034576416016, 33.88053512573242, 35.60791015625, 38.18766403198242, 39.805118560791016, 39.96106719970703, 38.19029235839844, 36.31589889526367, 35.59480667114258, 36.33930969238281, 39.25532913208008, 41.06071472167969, 39.25475311279297, 36.3889045715332, 35.14810562133789, 35.619361877441406, 35.607337951660156, 36.31424331665039, 37.345855712890625, 38.13763427734375, 38.530731201171875, 37.11848068237305, 35.943603515625, 35.7540168762207, 35.78102111816406, 35.6790885925293, 36.399688720703125, 37.58909606933594, 37.865020751953125, 37.394500732421875, 38.08434295654297, 37.00474548339844, 35.711143493652344, 34.3027458190918, 33.259525299072266, 34.1267204284668, 35.790287017822266, 37.55744934082031, 37.5158805847168, 36.84573745727539, 37.191986083984375, 36.658668518066406, 36.41597366333008, 36.130428314208984, 36.484100341796875, 37.68742370605469, 38.313270568847656, 37.811859130859375, 37.927669525146484, 35.144527435302734, 32.68762969970703, 32.17835235595703, 34.03058624267578, 35.85480499267578, 38.05647277832031, 37.84281921386719, 35.5022087097168, 33.92445373535156, 33.26036071777344, 33.543155670166016, 36.139442443847656, 37.669281005859375, 38.973934173583984, 37.69462966918945, 35.772586822509766, 36.00130844116211, 35.808349609375, 36.649044036865234, 38.39531326293945, 37.539676666259766, 37.03050231933594, 35.05109786987305, 34.480472564697266, 34.789215087890625, 36.681400299072266, 38.93830490112305, 40.137142181396484, 38.752052307128906, 36.38811111450195, 34.817378997802734, 35.66194534301758, 39.57133865356445, 41.30575180053711, 39.26139450073242, 35.419822692871094, 33.249271392822266, 32.54176330566406, 34.01225662231445, 36.98357391357422, 38.715423583984375, 37.87457275390625, 34.259891510009766, 32.628753662109375, 35.02288818359375, 40.20429611206055, 43.79283142089844, 42.33609390258789, 34.716575622558594, 31.327224731445312, 30.585927963256836, 31.98168182373047, 37.191707611083984, 42.2026252746582, 42.3109245300293, 38.108272552490234, 33.97248458862305, 32.3916015625, 31.946653366088867, 33.502227783203125, 36.60978317260742, 40.10654067993164, 41.41767501831055, 38.5870246887207, 36.53766632080078, 34.65230941772461, 35.76761245727539, 37.9353141784668, 40.153656005859375, 38.70899963378906, 35.968257904052734, 33.68500900268555, 33.8922233581543, 35.22993087768555, 37.76478576660156, 39.136234283447266, 38.5416259765625, 35.53068923950195, 32.932952880859375, 30.74807357788086, 30.202957153320312, 30.233978271484375, 31.959203720092773, 37.12173843383789, 41.728065490722656, 43.46464157104492, 39.4774055480957, 33.89500045776367, 32.04018783569336, 31.69669532775879, 32.853416442871094, 35.392921447753906, 35.66849899291992, 35.33157730102539, 32.470001220703125, 31.116830825805664, 34.144630432128906, 37.61746597290039, 38.011688232421875, 36.223079681396484, 33.95005798339844, 32.11066818237305, 32.01133728027344, 31.38374137878418, 32.154788970947266, 34.78367614746094, 38.20082092285156, 37.9605712890625, 34.79555892944336, 31.568574905395508, 30.87786102294922, 31.31117820739746, 33.74966812133789, 36.47187423706055, 35.316139221191406, 33.24102020263672, 31.69428062438965, 31.48538589477539, 33.664772033691406, 38.572898864746094, 39.74211502075195, 35.836219787597656, 32.848876953125]}],\n",
              "                        {\"height\": 720, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"width\": 960, \"xaxis\": {\"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"title\": {\"text\": \"Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('adbd32ea-0201-4254-90d4-e82e55732cac');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SfcFnOONyuNm"
      },
      "source": [
        "## Use the Keras Library to build an image recognition network using the Fashion-MNIST dataset (also comes with keras)\n",
        "\n",
        "- Load and preprocess the image data similar to how we preprocessed the MNIST data in class.\n",
        "- Make sure to one-hot encode your category labels\n",
        "- Make sure to have your final layer have as many nodes as the number of classes that you want to predict.\n",
        "- Try different hyperparameters. What is the highest accuracy that you are able to achieve.\n",
        "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
        "- Remember that neural networks fall prey to randomness so you may need to run your model multiple times (or use Cross Validation) in order to tell if a change to a hyperparameter is truly producing better results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "szi6-IpuzaH1",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zv_3xNMjzdLI"
      },
      "source": [
        "## Stretch Goals:\n",
        "\n",
        "- Use Hyperparameter Tuning to make the accuracy of your models as high as possible. (error as low as possible)\n",
        "- Use Cross Validation techniques to get more consistent results with your model.\n",
        "- Use GridSearchCV to try different combinations of hyperparameters. \n",
        "- Start looking into other types of Keras layers for CNNs and RNNs maybe try and build a CNN model for fashion-MNIST to see how the results compare."
      ]
    }
  ]
}